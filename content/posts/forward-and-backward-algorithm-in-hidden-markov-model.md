---
title: "Forward and Backward Algorithm in Hidden Markov Model"
date: 2021-10-26T00:09:35+02:00
draft: false
---

Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (`Evaluation`, `Learning` and `Decoding`).
In this post we'll deep dive into the **Evaluation Problem**. Starting from mathematical understanding, finishing on Python and R implementations.

# Quick recap

Hidden Markov Model is a **Markov Chain** which is mainly used in problems with **temporal sequence** of the data. Markov Model explaimns that the next step depends only on the previous step in
a temporal sequence. In Hidden Markov Model the state of the system is `hidden` however each state emits a visible symbol at every time step. HMM can work with both discrete and continuous sequence of the data
but we'll focus on the former.

## Basic structure of HMM

Hidden Markov Model ($\theta$) has the following parameters:

* Set of $M$ Hidden States ($S^M$)
* A Transaction Probability Matrix ($A$)
* A sequence of T observations ($V^T$)
* An Emission Probability Matrix ($B$)
* An initial probability distribution ($\pi$)

# Evaluation problem

As we have seen before, the Evaluation Problem can be stated as follows:

$$
\displaylines{
	\text{Given }  \theta, V_T   \rightarrow \text{Estimate } P(V_T|\theta) \\\\\\
	\text{Where }  \theta \rightarrow s, v, a_{ij},b_{jk}
}
$$

## Solution

1. First we need to find all possible sequences of the states $S^M$ where `M` is the number of Hidden States
2. Then from all those sequences of $S^M$, find the probability of which sequence generated the visible sequence of symbols $V^T$
3. Mathematically $P(V_T|\theta)$ can be estimated as $p(V^T|\theta)= \sum_{r=1}^{R} p(V^T|S_r^T)p(S_r^T), \\, \\, \text{where }S_r^T = \{ s_1(1), s_2(2)… s_r(T)\}$  
and $R$ - maximum number of possivle sequences of the hidden state.

If there are $M$ possible hidden states, then $R = M^T$.

In order to compute the probability of the model generated by the *particular* sequence of `T` visible symbols $V^T$, we should take each conceivable sequence of hidden states, calculate probability that they
have produced $V^T$ and then add up these probabilities.

## How to proof the solution is valid?

Let's present it in a different way.

Coming back to one of our previous examples, the diagram below presents a sequence of 3 states. The transition probabilities between hidden states are unknown.

![Transitions between hidden layers with unknown probabilities](/posts/images/3/transitions-visible.png)

In case in the example above we already know the sequence of the hidden states (`sun`, `sun`, `cloud`) which generated the 3 visible symbols (`happy`, `sad`, `happy`).

Calculation of the probability of the visible symbols given the hidden states is pretty straightforward.

$$
P(\text{happy, sad, happy} | \text{sun, sun, rain}) = P(\text{happy} | \text{sun}) \times P(\text{sad} | \text{sun}) \times P(\text{happy} | \text{rain}) 
$$

Mathematically probability of $V^T$ given $S^T$ can be written as:

$$
p(V^T|S_r^T)=\prod_{t=1}^{T} p(v(t) | s(t))
$$

Unfortunately we **don't know the specific sequence of hidden states** that generated the visible symbols. We need to compute the probability of mood changes 
(`happy`, `sad`, `happy`) by **summing over all possible weather sequences** weighted by their probability (transition probability).

![Transitions between hidden layers known with probabilities](/posts/images/3/transitions-hidden.png)

We can calculate the **joint probability** of the sequence of visible symbol $V^T$ generated by a specific sequence of hidden state $S^T$ as:

$$
\displaylines{
P(\text{happy, sad, happy} | \text{sun, sun, rain}) = P(\text{sun} | \text{initial state}) \times P(\text{sun} | \text{sun}) \times \\\\\\
\times P(\text{rain} | \text{sun})  \times P(\text{happy} | \text{sun}) \times P(\text{sad} | \text{sun}) \times P(\text{happy} | \text{rain}) 
}
$$

In general:

$$
P(V^T,S^T)=P(V^T | S^T)P(S^T)
$$

Since we're using **First-Order Markov Model**, we can say that the probability of a sequence of `T` hidden states is the multiplication of the probability of each transition:

$$
P(S^T)=\prod_{t=1}^{T} P(s(t) | s(t-1))
$$

So the joint probability can be written as:

$$\begin{eqnarray} 
    P(V^T,S^T) &=& P(V^T | S^T)P(S^T) \\\\\\
	&=& \prod_{t=1}^{T} P(v(t) | s(t)) \prod_{t=1}^{T} P(s(t) | s(t-1))
\end{eqnarray}$$

We are getting close to our original equation, just one more step is left. The above equation is for a **specific sequence** of hidden states that we thought could have generated
the visible sequence of symbols. We should compute the probability of **all** the different possible sequences of hidden states by **summing over all the joint probabilities** of
$V^T$ and $S^T$.

In out example, we have a sequence of 3 visible states and 2 hidden states that could emit them. There can be $2^3 = 8$ possible sequences.

The generalized equation:

$$
\begin{eqnarray} 
P(V^T|\theta) &=& \sum_{\text{all seq of S}} P(V^T, S^T) \\\\\\
                        &=& \sum_{\text{all seq of S}} P(V^T | S^T)P(S^T) \\\\\\
                       &=& \sum_{r=1}^R \prod_{t=1}^{T} P(v(t) | s(t)) \prod_{t=1}^{T} P(s(t) | s(t-1)) \\\\\\
                       &=& \sum_{r=1}^R \prod_{t=1}^{T} P(v(t) | s(t)) P(s(t) | s(t-1)) 
\end{eqnarray}
$$

As previously, $R$ - number of possible sequences of the hidden states.

A bad news is that the **computation complexity** of that approach is $O(N^T T)$ which makes it not really practical. We need to find easy to compute alternative and a **recursive dynamic programming**
is the one that can save us from the exponential computation. There are two such algorithms, `Forward Algorithm` and `Backward Algorithm`.


# Forward algorithm

As the name suggests, we want to use the computed probability on **current time step** to compute the probability of the **next time step**. The computational complexity is far more efficient than previous $O(N^T T)$.

To do this, we need to find the answer the following question - given a sequence of Visible states $V^T$ what is the probability that the Hidden Markov Model will be in a particular hidden state $s$ at a particular
time step $t$?

The question rewritten as mathematical formula:

$$
\alpha_j(t) = p(v(1)…v(t),s(t)= j) = ?
$$

We'll start with deriving the equation using probability and then solve it again using `trellis diagram`. If the former is too dificult, skip it and go straight to the diagram.

## Solution using probabilities

Before deriving a general equation, let's start with calculations for the small lengths of the sequence.

**When `t = 1`**  

$$
\begin{eqnarray} 
\alpha_j(1) &=& p(v_k(1),s(1)= j) \\\\\\
                   &=& p(v_k(1)|s(1)=j)p(s(1)=j) \\\\\\
                   &=& \pi_j p(v_k(1)|s(1)=j) \\\\\\
                   &=& \pi_j b_{jk} \\\\\\
\text{where } \pi &=& \text{ initial distribution, } \\\\\\
b_{jkv(1)} &=& \text{ emission probability at } t = 1 
\end{eqnarray}
$$

**When `t = 2`**  

We have the solution for `t=1`. Now we want to rewrite the same for `t=2` and come up with equation with $\alpha_j(1)$ as a part of it so we can use `recursion`.

$$
\begin{eqnarray} 
\alpha_j(2) &=& p \Big( v_k(1),v_k(2),s(2)= j \Big) \\\\\\
                   &=& {\sum_{i=1}^M} p \Big( v_k(1),v_k(2),{s(1)= i}, s(2)= j \Big) \\\\\\ 
                   &=&  \sum_{i=1}^M  p \Big( v_k(2) | s(2)= j, v_k(1),s(1)= i \Big) p \Big( v_k(1),s(2),s(1)= i \Big)\\\\\\
                   &=&  \sum_{i=1}^M  p \Big( v_k(2) | s(2)= j, {v_k(1), s(1)= i} \Big) p \Big( s(2) | {v_k(1),}s(1)= i \Big) p \Big(v_k(1),s(1)= i \Big) \\\\\\
                   &=&  \sum_{i=1}^M  p \Big( v_k(2) | s(2)= j  \Big) p \Big(s(2) | s(1)= i \Big) p \Big(v_k(1),s(1)= i \Big)\\\\\\
                   &=&  {p \Big( v_k(2) | s(2)= j \Big) }\sum_{i=1}^M p  \Big( s(2) | s(1)= i \Big) {p  \Big( v_k(1),s(1)= i \Big)} \\\\\\
                   &=&  {b_{jk v(2)}}  \sum_{i=1}^M a_{i2} {\alpha_i(1)}\\\\\\
\text{where } a_{i2} &=& \text{ transition probability } \\\\\\
b_{jk v(2)} &=& \text{ emission probability at } t=2 \\\\\\
\alpha_i(1) &=& \text{ forward probability at } t=1 
\end{eqnarray}
$$


## Generalized equation

Let's generalize the equation for any time step `t+1`:

$$
\begin{eqnarray} 
\alpha_j(t+1) &=& p \Big( v_k(1) … v_k(t+1),s(t+1)= j \Big)  \\\\\\
                   &=& {\sum_{i=1}^M} p\Big(v_k(1) … v_k(t+1),{s(t)= i}, s(t+1)= j \Big) \\\\\\
                   &=&  \sum_{i=1}^M  p\Big(v_k(t+1) | s(t+1)= j, v_k(1) … v_k(t),s(t)= i\Big) \cdot \\\\\\
				   &\cdot& p\Big(v_k(1)…v_k(t),s(t+1),s(t)= i \Big) = \\\\\\
                   &=&  \sum_{i=1}^M  p\Big(v_k(t+1) | s(t+1)= j, {v_k(1)…v_k(t), s(t)= i}\Big) \cdot \\\\\\
				   &\cdot& p\Big(s(t+1) | {v_k(1)…v_k(t),}s(t)= i\Big) p\Big(v_k(t),s(t)= i\Big)\\\\\\
                   &=&  \sum_{i=1}^M  p\Big(v_k(t+1) | s(t+1)= j\Big) p\Big(s(t+1) | s(t)= i\Big) p\Big(v_k(t),s(t)= i\Big)\\\\\\ 
                   &=&  {p\Big(v_k(t+1) | s(t+1)= j\Big) }\sum_{i=1}^M p\Big(s(t+1) | s(t)= i\Big) {p\Big(v_k(t),s(t)= i\Big)} \\\\\\
                   &=&  {b_{jk v(t+1)}} \sum_{i=1}^M a_{ij} {\alpha_i(t)} 
\end{eqnarray}
$$

The equation above follows the same derivation as for `t=2`. This equation will be really to implement using any programming language. We won't use
recursion function, just the pre-calculated values in a loop.

## Intuition using Trellis

We will use `Trellis Diagram` to get the intuition behind the Forward Algorithm. In case the derivation using joint probability rule was too dificult, 
this section will definitely help you to understand the equation.

Stating once again the question: given a sequence of visible states $V^T$ what will be the probability that the Hidden Markov Model will be in
particular hidden state $s$ at a particular time step $t$?

**Step by step derivation**

Look at the Trellis diagram below and assume the probability that the system/machine is at hidden state $s_1$ at time $(t-1)$ is $\alpha_1(t-1)$.
The probability of transition to hidden state $s_2$ at time step $t$ can be written as $\alpha_1(t-1)a_{12}$.

![Graph 3](/posts/images/7_trellis_forward.png)

Likewise, if we `sum` all the probabilities where the machine transitions to state $s_2$ at time `t` from any state at time $(t-1)$, it gives the total
probability that `there will be a transition from any hidden state` at $(t-1)$ to $s_2$ at time step `t`.

Mathematically:

$$
\sum_{i=1}^M \alpha_i(t-1) a_{i2}
$$

Finally, we can say the probability that the machine is at hidden state $s_2$ at time `t` after emitting first `t` number of visible symbols from
sequence $V^T$ is given by the following formula:

$$
b_{2k} \sum_{i=1}^M \alpha_i(t-1) a_{i2}
$$

Now we can extend this to a `recursive algorithm` to find the probability that sequence $V^T$ was generated by HMM $\theta$. Here is the generalized
version of the equation.

$$
\alpha_j(t)= \begin{cases} 
\pi_jb_{jk} & \text{ when }t = 1 \\\\\\
b_{jk} \sum_{i=1}^M \alpha_i(t-1) a_{ij} & \text{ when } t \text{ greater than } 1 
\end{cases}
$$

Here $\alpha_j(t)$ is the probability that the machine will be at hidden state $s_j$ at time step `t` after emitting first `t` visible sequence of symbols.

## Implementation of Forward algorithm

Now let's work on the implementation. We'll use both Python and R for this.

**Data**

In our example we have 2 Hidden States `(A, B)` and 3 Visible States `(0, 1, 2)` (in R file it will be `(1, 2, 3)`). Assume that we already know our `a` and `b`.

$$
A= 
\begin{bmatrix} 
0.54 & 0.46\\\\\\ 
0.49 & 0.51 
\end{bmatrix}
$$

$$
B= \begin{bmatrix} 
0.16 & 0.26 & 0.58\\\\\\
0.25 & 0.28 & 0.47 
\end{bmatrix}
$$

The `data_python.csv` and `data_r.csv` has two columns named `Hidden` and `Visible`. The only difference between the Python and R is only the starting index of the Visible column.
Python file has 0, 1, 2 where R has 1, 2, 3.

**Python**

First load the data.

```python
import pandas as pd
import numpy as np
 
data = pd.read_csv('data_python.csv')
 
V = data['Visible'].values
```

Then set the values for transition probability, emission probabilities and initial distribution.

```python
# Transition Probabilities
a = np.array(((0.54, 0.46), (0.49, 0.51)))
 
# Emission Probabilities
b = np.array(((0.16, 0.26, 0.58), (0.25, 0.28, 0.47)))
 
# Equal Probabilities for the initial distribution
initial_distribution = np.array((0.5, 0.5))
```

In python the index starts from 0, hence our `t` will start from `0` to `T-1`.

Next, we will have the `forward function`. Here we will store and return all the $\alpha_0(0), \\ \alpha_1(0) \\ …  \\ \alpha_0(T-1), \\ \alpha_1(T-1)$

```python
def forward(V, a, b, initial_distribution):
    alpha = np.zeros((V.shape[0], a.shape[0]))
    alpha[0, :] = initial_distribution * b[:, V[0]]
 
    for t in range(1, V.shape[0]):
        for j in range(a.shape[0]):
            # Matrix Computation Steps
            #                  ((1x2) . (1x2))      *     (1)
            #                        (1)            *     (1)
            alpha[t, j] = alpha[t - 1].dot(a[:, j]) * b[j, V[t]]
 
    return alpha
 
alpha = forward(V, a, b, initial_distribution)
print(alpha)
```

First we will create the alpha matrix with `2 columns` and `T Rows`.  
As per our equation multiply `initial_distribution` with the $b_{jkv(0)}$ to calculate $\alpha_0(0) , \\ \alpha_1(0)$. This will be a simple vector multiplication since both `initial_distribution` and $b_{kv(0)}$
are of same size.

* We will loop through the time steps now, starting from 1 (remember python index starts from 0).
* Another loop for each hidden step `j`.
* Use the same formula for calculating the $\alpha$ values.
* Return all of the alpha values.

**Output**

```python
[[8.00000000e-002 1.25000000e-001]
[[8.00000000e-002 1.25000000e-001]
 [2.71570000e-002 2.81540000e-002]
 [1.65069392e-002 1.26198572e-002]
 [8.75653677e-003 6.59378003e-003]
…
…
 [8.25847348e-221 6.30684489e-221]
 [4.37895921e-221 3.29723269e-221]
 [1.03487332e-221 1.03485477e-221]
 [6.18228050e-222 4.71794300e-222]]
```
  
  
**R**

Here is the same Forward Algorithm implemented in R. If you notice, we have removed the 2nd for loop in R code. You can do the same in python.

```r
data = read.csv("data_r.csv")
 
a = matrix(c(0.54, 0.49, 0.46, 0.51),nrow = 2,ncol = 2)
b = matrix(c(0.16, 0.25, 0.26, 0.28, 0.58, 0.47),nrow = 2,ncol = 3)
initial_distribution = c(1/2, 1/2)
 
forward = function(v, a, b, initial_distribution){
  
  T = length(v)
  m = nrow(a)
  alpha = matrix(0, T, m)
  
  alpha[1, ] = initial_distribution*b[, v[1]]
  
  for(t in 2:T){
    tmp = alpha[t-1, ] %*% a
    alpha[t, ] = tmp * b[, v[t]]
  }
  return(alpha)
}
 
forward(data$Visible,a,b,initial_distribution)
```

# Backward algorithm

Backward Algorithm is the `time-reversed` version of the Forward Algorithm. In Backward Algorithm we need to find the probability that the machine will be in hidden state $s_i$ at time step `t`
and will generate the `remaining part of the sequence` of the visible symbol $V^T$.

## Derivation of Backward algorithm

The concepts are same as when we were deriving the forward algorithm using Probability Theory.

$$
\begin{eqnarray} 
\beta_i(t) &=& p \Big( v_k(t+1) …. v_k(T) | s(t) = i \Big) \\\\\\ 
				&=& \sum_{j=0}^M p\Big(  v_k(t+1) …. v_k(T), s(t+1) = j | s(t) = i  \Big)        \\\\\\ 
				&=& \sum_{j=0}^M p\Big(  v_k(t+2) …. v_k(T) |  v_k(t+1) , s(t+1) = j , s(t) = i  \Big) \cdot \\\\\\ 
				&\cdot& p \Big( v_k(t+1) , s(t+1) = j | s(t) = i  \Big) \\\\\\ 
				&=& \sum_{j=0}^M p\Big(  v_k(t+2) …. v_k(T) |  v_k(t+1) , s(t+1) = j , s(t) = i  \Big) \cdot \\\\\\ 
				&\cdot&	p \Big( v_k(t+1) |  s(t+1) = j , s(t) = i  \Big) p \Big( s(t+1) = j | s(t) = i  \Big) \\\\\\ 
				&=& \sum_{j=0}^M  p\Big(  v_k(t+2) …. v_k(T) |  s(t+1) = j  \Big) p \Big( v_k(t+1) |  s(t+1) = j \Big) \cdot \\\\\\
				&\cdot& p \Big( s(t+1) = j | s(t) = i  \Big) \\\\\\ 
				&=& \sum_{j=0}^M \beta_j(t+1) b_{jkv(t+1)} a_{ij} \\\\\\ 
\text{where } a_{i2} &=& \text{ Transition Probability } \\\\\\ 
b_{jk v(t+1)} &=& \text{ Emission Probability at } t=t+1 \\\\\\ 
\beta_i(t+1) &=& \text{ Backward probability at } t=t+1 
\end{eqnarray}
$$

## Intuition using Trellis

Here is the Trellis diagram of the Backward Algorithm. Mathematically, the algorithm can be written in following way:

$$
\beta_i(t)= \begin{cases} 
1 & \text{ when }t = T \\\\\\ 
\sum_{j=0}^M  a_{ij} b_{jkv(t+1)}\beta_j(t+1) & \text{ when } t \text{ less than } T 
\end{cases}
$$

![Graph 4](/posts/images/8_trellis_backward.png)

## Implementation of Backward algorithm

We will use the same data file and parameters as defined for Forward Algorithm.

**Python code**

```python
import pandas as pd
import numpy as np
 
data = pd.read_csv('data_python.csv')
 
V = data['Visible'].values
 
# Transition Probabilities
a = np.array(((0.54, 0.46), (0.49, 0.51)))
 
# Emission Probabilities
b = np.array(((0.16, 0.26, 0.58), (0.25, 0.28, 0.47)))
 
 
def backward(V, a, b):
    beta = np.zeros((V.shape[0], a.shape[0]))
 
    # setting beta(T) = 1
    beta[V.shape[0] - 1] = np.ones((a.shape[0]))
 
    # Loop in backward way from T-1 to
    # Due to python indexing the actual loop will be T-2 to 0
    for t in range(V.shape[0] - 2, -1, -1):
        for j in range(a.shape[0]):
            beta[t, j] = (beta[t + 1] * b[:, V[t + 1]]).dot(a[j, :])
 
    return beta
 
 
beta = backward(V, a, b)
print(beta)
```

**R code**

```r
data = read.csv("data_r.csv")
 
a = matrix(c(0.54, 0.49, 0.46, 0.51),nrow = 2,ncol = 2)
b = matrix(c(0.16, 0.25, 0.26, 0.28, 0.58, 0.47),nrow = 2,ncol = 3)
 
backward = function(V, A, B){
  T = length(V)
  m = nrow(A)
  beta = matrix(1, T, m)
  
  for(t in (T-1):1){
    tmp = as.matrix(beta[t+1, ] * B[, V[t+1]])
    beta[t, ] = t(A %*% tmp)
  }
  return(beta)
}
 
backward(data$Visible,a,b)
```

**Output**

```python
[[5.30694627e-221 5.32373319e-221]
 [1.98173335e-220 1.96008747e-220]
 [3.76013005e-220 3.71905927e-220]
 [7.13445025e-220 7.05652279e-220]
...
...
 [7.51699476e-002 7.44006456e-002]
 [1.41806080e-001 1.42258480e-001]
 [5.29400000e-001 5.23900000e-001]
 [1.00000000e+000 1.00000000e+000]]
```

# Conclusion

In the next article we will use both forward and backward algorithm to solve the `learning problem`. Here we have provided a very detailed overview of the Forward and Backward Algorithm.
The output of the program may not make a lot of sense now however the next article will provide more insight.
