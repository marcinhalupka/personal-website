---
title: "Forward and Backward Algorithm in Hidden Markov Model"
date: 2021-10-26T00:09:35+02:00
draft: false
---

Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (`Evaluation`, `Learning` and `Decoding`).
In this post we'll deep dive into the **Evaluation Problem**. Starting from mathematical understanding, finishing on Python and R implementations.

# Quick recap

Hidden Markov Model is a **Markov Chain** which is mainly used in problems with **temporal sequence** of the data. Markov Model explaimns that the next step depends only on the previous step in
a temporal sequence. In Hidden Markov Model the state of the system is `hidden` however each state emits a visible symbol at every time step. HMM can work with both discrete and continuous sequence of the data
but we'll focus on the former.

## Basic structure of HMM

Hidden Markov Model ($\theta$) has the following parameters:

* Set of $M$ Hidden States ($S^M$)
* A Transaction Probability Matrix ($A$)
* A sequence of T observations ($V^T$)
* An Emission Probability Matrix ($B$)
* An initial probability distribution ($\pi$)

# Evaluation problem

As we have seen before, the Evaluation Problem can be stated as follows:

$$
\displaylines{
	\text{Given }  \theta, V_T   \rightarrow \text{Estimate } P(V_T|\theta) \\\\\\
	\text{Where }  \theta \rightarrow s, v, a_{ij},b_{jk}
}
$$

## Solution

1. First we need to find all possible sequences of the states $S^M$ where `M` is the number of Hidden States
2. Then from all those sequences of $S^M$, find the probability of which sequence generated the visible sequence of symbols $V^T$
3. Mathematically $P(V_T|\theta)$ can be estimated as $p(V^T|\theta)= \sum_{r=1}^{R} p(V^T|S_r^T)p(S_r^T), \\, \\, \text{where }S_r^T = \{ s_1(1), s_2(2)… s_r(T)\}$  
and $R$ - maximum number of possivle sequences of the hidden state.

If there are $M$ possible hidden states, then $R = M^T$.

In order to compute the probability of the model generated by the *particular* sequence of `T` visible symbols $V^T$, we should take each conceivable sequence of hidden states, calculate probability that they
have produced $V^T$ and then add up these probabilities.

## How to proof the solution is valid?

Let's present it in a different way.

Coming back to one of our previous examples, the diagram below presents a sequence of 3 states. The transition probabilities between hidden states are unknown.

![Transitions between hidden layers with unknown probabilities](/posts/images/3/transitions-visible.png)

In case in the example above we already know the sequence of the hidden states (`sun`, `sun`, `cloud`) which generated the 3 visible symbols (`happy`, `sad`, `happy`).

Calculation of the probability of the visible symbols given the hidden states is pretty straightforward.

$$
P(\text{happy, sad, happy} | \text{sun, sun, rain}) = P(\text{happy} | \text{sun}) \times P(\text{sad} | \text{sun}) \times P(\text{happy} | \text{rain}) 
$$

Mathematically probability of $V^T$ given $S^T$ can be written as:

$$
p(V^T|S_r^T)=\prod_{t=1}^{T} p(v(t) | s(t))
$$

Unfortunately we **don't know the specific sequence of hidden states** that generated the visible symbols. We need to compute the probability of mood changes 
(`happy`, `sad`, `happy`) by **summing over all possible weather sequences** weighted by their probability (transition probability).

![Transitions between hidden layers known with probabilities](/posts/images/3/transitions-hidden.png)

We can calculate the **joint probability** of the sequence of visible symbol $V^T$ generated by a specific sequence of hidden state $S^T$ as:

$$
\displaylines{
P(\text{happy, sad, happy} | \text{sun, sun, rain}) = P(\text{sun} | \text{initial state}) \times P(\text{sun} | \text{sun}) \times \\\\\\
\times P(\text{rain} | \text{sun})  \times P(\text{happy} | \text{sun}) \times P(\text{sad} | \text{sun}) \times P(\text{happy} | \text{rain}) 
}
$$

In general:

$$
P(V^T,S^T)=P(V^T | S^T)P(S^T)
$$

Since we're using **First-Order Markov Model**, we can say that the probability of a sequence of `T` hidden states is the multiplication of the probability of each transition:

$$
P(S^T)=\prod_{t=1}^{T} P(s(t) | s(t-1))
$$

So the joint probability can be written as:

$$\begin{eqnarray} 
    P(V^T,S^T) &=& P(V^T | S^T)P(S^T) \\\\\\
	&=& \prod_{t=1}^{T} P(v(t) | s(t)) \prod_{t=1}^{T} P(s(t) | s(t-1))
\end{eqnarray}$$

We are getting close to our original equation, just one more step is left. The above equation is for a **specific sequence** of hidden states that we thought could have generated
the visible sequence of symbols. We should compute the probability of **all** the different possible sequences of hidden states by **summing over all the joint probabilities** of
$V^T$ and $S^T$.

In out example, we have a sequence of 3 visible states and 2 hidden states that could emit them. There can be $2^3 = 8$ possible sequences.

The generalized equation:

$$
\begin{eqnarray} 
P(V^T|\theta) &=& \sum_{\text{all seq of S}} P(V^T, S^T) \\\\\\
                        &=& \sum_{\text{all seq of S}} P(V^T | S^T)P(S^T) \\\\\\
                       &=& \sum_{r=1}^R \prod_{t=1}^{T} P(v(t) | s(t)) \prod_{t=1}^{T} P(s(t) | s(t-1)) \\\\\\
                       &=& \sum_{r=1}^R \prod_{t=1}^{T} P(v(t) | s(t)) P(s(t) | s(t-1)) 
\end{eqnarray}
$$

As previously, $R$ - number of possible sequences of the hidden states.

A bad news is that the **computation complexity** of that approach is $O(N^T T)$ which makes it not really practical. We need to find easy to compute alternative and a **recursive dynamic programming**
is the one that can save us from the exponential computation. There are two such algorithms, `Forward Algorithm` and `Backward Algorithm`.


# Forward algorithm

As the name suggests, we want to use the computed probability on **current time step** to compute the probability of the **next time step**. The computational complexity is far more efficient than previous $O(N^T T)$.

To do this, we need to find the answer the following question - given a sequence of Visible states $V^T$ what is the probability that the Hidden Markov Model will be in a particular hidden state $s$ at a particular
time step $t$?

The question rewritten as mathematical formula:

$$
\alpha_j(t) = p(v(1)…v(t),s(t)= j) = ?
$$

We'll start with deriving the equation using probability and then solve it again using `trellis diagram`. If the former is too dificult, skip it and go straight to the diagram.

## Solution using probabilities

Before deriving a general equation, let's start with calculations for the small lengths of the sequence.

**When `t = 1`**  

$$
\begin{eqnarray} 
\alpha_j(1) &=& p(v_k(1),s(1)= j) \\\\\\
                   &=& p(v_k(1)|s(1)=j)p(s(1)=j) \\\\\\
                   &=& \pi_j p(v_k(1)|s(1)=j) \\\\\\
                   &=& \pi_j b_{jk} \\\\\\
\text{where } \pi &=& \text{ initial distribution, } \\\\\\
b_{jkv(1)} &=& \text{ emission probability at } t = 1 
\end{eqnarray}
$$

**When `t = 2`**  

We have the solution for `t=1`. Now we want to rewrite the same for `t=2` and come up with equation with $\alpha_j(1)$ as a part of it so we can use `recursion`.

$$
\begin{eqnarray} 
\alpha_j(2) &=& p \Big( v_k(1),v_k(2),s(2)= j \Big) \\\\\\
                   &=& {\sum_{i=1}^M} p \Big( v_k(1),v_k(2),{s(1)= i}, s(2)= j \Big) \\\\\\ 
                   &=&  \sum_{i=1}^M  p \Big( v_k(2) | s(2)= j, v_k(1),s(1)= i \Big) p \Big( v_k(1),s(2),s(1)= i \Big)\\\\\\
                   &=&  \sum_{i=1}^M  p \Big( v_k(2) | s(2)= j, {v_k(1), s(1)= i} \Big) p \Big( s(2) | {v_k(1),}s(1)= i \Big) p \Big(v_k(1),s(1)= i \Big) \\\\\\
                   &=&  \sum_{i=1}^M  p \Big( v_k(2) | s(2)= j  \Big) p \Big(s(2) | s(1)= i \Big) p \Big(v_k(1),s(1)= i \Big)\\\\\\
                   &=&  {p \Big( v_k(2) | s(2)= j \Big) }\sum_{i=1}^M p  \Big( s(2) | s(1)= i \Big) {p  \Big( v_k(1),s(1)= i \Big)} \\\\\\
                   &=&  {b_{jk v(2)}}  \sum_{i=1}^M a_{i2} {\alpha_i(1)}\\\\\\
\text{where } a_{i2} &=& \text{ transition probability } \\\\\\
b_{jk v(2)} &=& \text{ emission probability at } t=2 \\\\\\
\alpha_i(1) &=& \text{ forward probability at } t=1 
\end{eqnarray}
$$


## Generalized equation

Placeholder for text.

## Intuition using Trellis

Placeholder for text.

## Implementation of Forward algorithm

Placeholder for text.

# Backward algorithm

Placeholder for text.

## Derivation of Backward algorithm

Placeholder for text.

## Intuition using Trellis

Placeholder for text.

## Implementation of Backward algorithm

Placeholder for text.

# Conclusion

Placeholder for text.
