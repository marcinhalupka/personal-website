<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Introduction to Hidden Markov Model | Marcin Halupka</title>
<meta name=keywords content>
<meta name=description content="The goal of this post and series is to present the foundational concept, usability, intuition of the algorithmic part and some basic axamples of Hidden Markov Models. To understand the content, basic knowledge on probability should be sufficient.
Why kind of problems I can solve with them? The ones where we observe a sequential data but we suspect that behind the scenes there may be something going on that affects the sequence we are looking at.">
<meta name=author content="Marcin Halupka">
<link rel=canonical href=/posts/introduction-to-hidden-markov-model/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css integrity="sha256-b2AFbUTT9+tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=16x16 href=%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=%3Clink%20/%20abs%20url%3E>
<link rel=apple-touch-icon href=%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="Introduction to Hidden Markov Model">
<meta property="og:description" content="The goal of this post and series is to present the foundational concept, usability, intuition of the algorithmic part and some basic axamples of Hidden Markov Models. To understand the content, basic knowledge on probability should be sufficient.
Why kind of problems I can solve with them? The ones where we observe a sequential data but we suspect that behind the scenes there may be something going on that affects the sequence we are looking at.">
<meta property="og:type" content="article">
<meta property="og:url" content="/posts/introduction-to-hidden-markov-model/"><meta property="og:image" content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-10-22T00:00:00+00:00">
<meta property="article:modified_time" content="2021-10-22T00:00:00+00:00"><meta property="og:site_name" content="ExampleSite">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="Introduction to Hidden Markov Model">
<meta name=twitter:description content="The goal of this post and series is to present the foundational concept, usability, intuition of the algorithmic part and some basic axamples of Hidden Markov Models. To understand the content, basic knowledge on probability should be sufficient.
Why kind of problems I can solve with them? The ones where we observe a sequential data but we suspect that behind the scenes there may be something going on that affects the sequence we are looking at.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"/posts/"},{"@type":"ListItem","position":3,"name":"Introduction to Hidden Markov Model","item":"/posts/introduction-to-hidden-markov-model/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introduction to Hidden Markov Model","name":"Introduction to Hidden Markov Model","description":"The goal of this post and series is to present the foundational concept, usability, intuition of the algorithmic part and some basic axamples of Hidden Markov Models. To understand the content, basic knowledge on probability should be sufficient.\nWhy kind of problems I can solve with them? The ones where we observe a sequential data but we suspect that behind the scenes there may be something going on that affects the sequence we are looking at.","keywords":[],"articleBody":"The goal of this post and series is to present the foundational concept, usability, intuition of the algorithmic part and some basic axamples of Hidden Markov Models. To understand the content, basic knowledge on probability should be sufficient.\nWhy kind of problems I can solve with them? The ones where we observe a sequential data but we suspect that behind the scenes there may be something going on that affects the sequence we are looking at. Weather, text, speech and biological sequences could be a great candidates.\nLet’s look at a simple example with a dishonest casino. The game we are playing is to guess the outcone of a dice roll and the dice has 6 sides. As we told, casino is not playing fair and is using two dices - one of them is fair and another one unfair. Unfair means that the probabilities of the outcomes are different than (1/6, 1/6, 1/6, 1/6, 1/6, 1/6) - they are not uniform. We know the outcome of the dice roll (1 to 6), we know the sequence of the throws (obsevations) but we can’t tell when the unfair die was used (hidden state). Hidden Markov Model can give us the answer to this question.\nThe intuition of Markov Models Markov Models are used to model randomly changing systems, like weather patterns. In Markov Model all the states are visible/observable.\nThe key assumption of Markov Model is that the future state/event depends only on current state/event and not on any other states (Markov Property). If we consider weather pattern (sunny, rainy, cloudy) then we can say tomorrow’s weather will only depend on today’s weather and not on the weather from previous n days.\nMathematically speaking, the probability of the state at time t will only depend on time step t-1, in other words $P(s(t)|s(t-1))$. This is known as First Order Markov Model.\nIf the probability of the state at time t depends on time step t-1 and t-2, the order increases and we receive $P(s(t)|s(t-1), s(t-2))$ - Second Order Markov Model. If we need to increase the dependency of past time events, the recipe for higher orders is clear.\nEventually, te idea is to model the joint probability of a sequence, such as the probability of $(s_1, s_2, s_3)$ where $s_1$, $s_2$ and $s_3$ happened one after another. We can use the join and conditional probability rule and write it as:\n$$\\begin{eqnarray} P(s_3,s_2,s_1) \u0026=\u0026 P(s_3|s_2,s_1)P(s_2,s_1) \\\\\\\n\u0026=\u0026 P(s_3|s_2,s_1)P(s_2|s_1)P(s_1) \\\\\\\n\u0026=\u0026 P(s_3|s_2)P(s_2|s_1)P(s_1) \\end{eqnarray}$$\nThe diagram for this simple Markov Model:\nTransition probabilities The probability of one state changing to another state is defined as transition probability. So in case of having 3 states (Sun, Cloud, Rain), there will be total 9 transition probabilities. As you look at the diagram, all transition probabilities were defined. Transition probability generally are denoted as $a_{ij}$ which can be interpreted as the probability of the system to transition from state i to state j.\n$$ a_{ij} = P(s(t+1) = j | s(t) = i) $$\nFor an example presented above, the transition probability from Sun to Cloud is defined as $a_{12}$. Note that the transition might happen to the same state also. If we have sun in two consecutive days then the transition probability from sun to sun at the next time step will be $a_{11}$.\nGenerally, the transition probabilities are defined using a (M x M) matrix, known as Transition Probability Matrix. For our example, the Transition Probability Matrix would be defined as:\n$$ A = \\begin{bmatrix}a_{11} \u0026 a_{12} \u0026 a_{13} \\\\\\ a_{21} \u0026 a_{22} \u0026 a_{23} \\\\\\ a_{31} \u0026 a_{32} \u0026 a_{33} \\end{bmatrix} $$\nNice property - when the system transitions to another state, the sum of all transition probabilities given the current state should be equal to 1, so for example $a_{11} + a_{12} + a_{13} = 1$.\nIn general:\n$$ \\sum_{j=1}^{M} a_{ij} = 1 \\; \\; \\; \\forall i $$\nInitial probability distribution The system has to start from one state. The initial state of Markov Model (when time step t = 0) is denoted as $\\pi$ - N dimensional row vector. All the probabilities must sum to 1, so $ \\sum_{i=1}^{N} \\pi_i = 1 \\; \\; \\; \\forall i$.\nDuring implementation, we can assume the initial probability distribution is uniform. In our weather example, we could define initial state as $ \\pi = [ \\frac{1}{3} \\frac{1}{3} \\frac{1}{3}]$.\nIn some cases we may have $\\pi_i = 0$, if particular state can’t be an initial one.\nMarkov Chain When the system is fully observable and autonomous, it’s called a Markov Chain. What we’ve learned so far is a great example.\nWe can conclude that Markov Chain consists of the following parameters:\n A set of N states A transition probability matrix A An initial probability distribution $\\pi$  Absorbing state As the name suggests, when transition probabilities from one state to all the other except for itself are 0, then it’s called an Absorbing State. When the system enters into that state, it never leaves.\nHidden Markov Model In Hidden Markov Model the state of the system will be hidden (unknown), however at every time step t the system in state s(t) will emit an observable symbol v(t).\nThe idea is presented on a graph below:\nIn our initial example of dishonest casino, the die rolled (fair or unfair) is hidden. However every time the die is rolled, we know the outcome (number between 1 and 6) - this is the symbol we observe.\nSummary:\n We can define a particular sequence of visible states/symbols as $V^T = { v(1), v(2) … v(T) }$ Then define model $\\theta$, so for every state s(t) we have a probability of emitting a particular visible state $v_k(t)$ Since we have access to the visible states only (s(t)'s are unobservable), a model is called a Hidden Markov Model  Emission probability We can redefine our previous example. Assume based on the weather of any day the mood of a person changes from happy to sad. Also assume the person is at a remote place and we don’t know how the weather is there. We can only know the mood of the person. In this scenario, the weather is the hidden state in the model and the mood (happy/sad) is the visible state. So we can use the HMM to predict the weather, knowing the mood of the person.\nThe graphical presentation of transition mechanism looks like that:\nThe observable symbols are ${ v_1 , v_2 }$ - from each hidden state one of them must be emitted. The probability of emitting a symbol is known as emission probability - defined by us as $b_{jk}$. Mathematically, it is the probability of emitting symbol k given state j:\n$$ b_{jk} = p(v_k(t) | s_j(t) ) $$\nEmission probabilities are also defined using N x M matrix, called Emission Probability Matrix:\n$$ B = \\begin{bmatrix} b_{11} \u0026 b_{12} \\\\\\ b_{21} \u0026 b_{22} \\\\\\\nb_{31} \u0026 b_{32} \\end{bmatrix} $$\nThe Emission Probabilities, same as Transition Probabilities, sum to 1.\n$$ \\sum_{k=1}^{C} b_{jk} = 1, \\; \\; \\; \\forall j $$\nWe have the attributes and properties of our Hidden Markov Model defined. Few more obstacles are waiting for us before reaching our ultimate goal for a new tool - prediction.\nCentral issues with Hidden Markov Model Evaluation problems We can define the model $\\theta$ as following:\n$$ \\theta \\rightarrow s, v, a_{ij},b_{jk} $$\nGiven the model ($\\theta$) and sequence of visible states ($V^T$), we need to determine the probability that a particular sequence of visible states was generated from the model.\nThere could be many models ${ \\theta_1, \\theta_2 … \\theta_n }$. We need to find $p(V^T | \\theta_i)$ then use Bayes Rule to correctly classify the sequence $V^T$.\n$$ p(\\theta | V^T ) = \\frac{p(V^T | \\theta) p(\\theta)}{p(V^T)} $$\nLearning problems In general HMM is unsupervised learning process - number of different visible symbol types is known (like happy/sad), but the number of hidden states is not. We can always try out different options however this may result in significant amount of computation. Picking the number of hidden states can be a more or less educated guess.\nOnce we have the high level structure (number of hidden and visible states) of the model, we need to estimate the transition ($a_{ij}$) and emission ($b_{jk}$) probabilities using the training sequence of visible states - that’s known as a learning problem.\nWe’ll be using the evaluation problem to solve the learning problem, so understanding the former is necessary. The transition and emission probabilities will be estimated with use of the Expectation Maximization algorithm. Entire learning problem is known as Forward-Backward Algorithm or Baum-Welch Algorithm.\nDecoding problem Once we have the estimates for transition ($a_{ij}$) and emission ($b_{jk}$) probabilities, we can use the model ($\\theta$) to predict the hidden states ($W^T$) which generated the visible sequence ($V^T$). The decoding problem is known as Viterbi Algorithm.\nConclusion The introduction gives some intuition about the Hidden Markov Models. To implement the algorithm from scratch, we have to go through three problems defined above as the next steps.\n","wordCount":"1506","inLanguage":"en","datePublished":"2021-10-22T00:00:00Z","dateModified":"2021-10-22T00:00:00Z","author":{"@type":"Person","name":"Marcin Halupka"},"mainEntityOfPage":{"@type":"WebPage","@id":"/posts/introduction-to-hidden-markov-model/"},"publisher":{"@type":"Organization","name":"Marcin Halupka","logo":{"@type":"ImageObject","url":"%3Clink%20/%20abs%20url%3E"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href accesskey=h title="Marcin Halupka (Alt + H)">Marcin Halupka</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=/posts/ title=Posts>
<span>Posts</span>
</a>
</li>
<li>
<a href=/projects/ title=Projects>
<span>Projects</span>
</a>
</li>
<li>
<a href=/about/ title="About me">
<span>About me</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href>Home</a>&nbsp;»&nbsp;<a href=/posts/>Posts</a></div>
<h1 class=post-title>
Introduction to Hidden Markov Model
</h1>
<div class=post-meta>October 22, 2021&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Marcin Halupka&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/introduction-to-hidden-markov-model.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header> <div class=toc>
<details open>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#why-kind-of-problems-i-can-solve-with-them aria-label="Why kind of problems I can solve with them?">Why kind of problems I can solve with them?</a></li>
<li>
<a href=#the-intuition-of-markov-models aria-label="The intuition of Markov Models">The intuition of Markov Models</a><ul>
<li>
<a href=#transition-probabilities aria-label="Transition probabilities">Transition probabilities</a></li>
<li>
<a href=#initial-probability-distribution aria-label="Initial probability distribution">Initial probability distribution</a></li>
<li>
<a href=#markov-chain aria-label="Markov Chain">Markov Chain</a></li>
<li>
<a href=#absorbing-state aria-label="Absorbing state">Absorbing state</a></li></ul>
</li>
<li>
<a href=#hidden-markov-model aria-label="Hidden Markov Model">Hidden Markov Model</a><ul>
<li>
<a href=#emission-probability aria-label="Emission probability">Emission probability</a></li></ul>
</li>
<li>
<a href=#central-issues-with-hidden-markov-model aria-label="Central issues with Hidden Markov Model">Central issues with Hidden Markov Model</a><ul>
<li>
<a href=#evaluation-problems aria-label="Evaluation problems">Evaluation problems</a></li>
<li>
<a href=#learning-problems aria-label="Learning problems">Learning problems</a></li>
<li>
<a href=#decoding-problem aria-label="Decoding problem">Decoding problem</a></li></ul>
</li>
<li>
<a href=#conclusion aria-label=Conclusion>Conclusion</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p>The goal of this post and series is to present the foundational concept, usability, intuition of the algorithmic part and some basic axamples of Hidden Markov Models.
To understand the content, basic knowledge on probability should be sufficient.</p>
<h1 id=why-kind-of-problems-i-can-solve-with-them>Why kind of problems I can solve with them?<a hidden class=anchor aria-hidden=true href=#why-kind-of-problems-i-can-solve-with-them>#</a></h1>
<p>The ones where we observe a sequential data but we suspect that behind the scenes there may be something going on that affects the sequence we are looking at.
Weather, text, speech and biological sequences could be a great candidates.</p>
<p>Let&rsquo;s look at a simple example with a dishonest casino. The game we are playing is to guess the outcone of a dice roll and the dice has 6 sides.
As we told, casino is not playing fair and is using two dices - one of them is fair and another one unfair. Unfair means that the probabilities of the outcomes are different than
<code>(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)</code> - they are not uniform. We know the outcome of the dice roll (1 to 6), we know the sequence of the throws (obsevations) but we can&rsquo;t tell when the unfair die was used (hidden state).
Hidden Markov Model can give us the answer to this question.</p>
<h1 id=the-intuition-of-markov-models>The intuition of Markov Models<a hidden class=anchor aria-hidden=true href=#the-intuition-of-markov-models>#</a></h1>
<p>Markov Models are used to model randomly changing systems, like weather patterns. In Markov Model all the states are visible/observable.</p>
<p>The key assumption of Markov Model is that <strong>the future state/event depends only on current state/event</strong> and not on any other states (<code>Markov Property</code>). If we consider weather pattern (sunny, rainy, cloudy)
then we can say tomorrow&rsquo;s weather will only depend on today&rsquo;s weather and not on the weather from previous n days.</p>
<p>Mathematically speaking, the probability of the <code>state</code> at time <code>t</code> will only depend on time step <code>t-1</code>, in other words $P(s(t)|s(t-1))$. This is known as <code>First Order Markov Model</code>.</p>
<p>If the probability of the <code>state</code> at time <code>t</code> depends on time step <code>t-1</code> and <code>t-2</code>, the order increases and we receive $P(s(t)|s(t-1), s(t-2))$ - <code>Second Order Markov Model</code>.
If we need to increase the dependency of past time events, the recipe for higher orders is clear.</p>
<p>Eventually, te idea is to model the joint probability of a sequence, such as the probability of $(s_1, s_2, s_3)$ where $s_1$, $s_2$ and $s_3$ happened one after another.
We can use the join and conditional probability rule and write it as:</p>
<p>$$\begin{eqnarray}
P(s_3,s_2,s_1) &=& P(s_3|s_2,s_1)P(s_2,s_1) \\\<br>
&=& P(s_3|s_2,s_1)P(s_2|s_1)P(s_1) \\\<br>
&=& P(s_3|s_2)P(s_2|s_1)P(s_1)
\end{eqnarray}$$</p>
<p>The diagram for this simple Markov Model:</p>
<p><img loading=lazy src=/posts/images/introduction-to-hidden-markov-model/1_transitions.png alt="Markov Model Diagram">
</p>
<h2 id=transition-probabilities>Transition probabilities<a hidden class=anchor aria-hidden=true href=#transition-probabilities>#</a></h2>
<p>The probability of one state changing to another state is defined as <code>transition probability</code>. So in case of having 3 states <code>(Sun, Cloud, Rain)</code>, there will be total 9 transition probabilities.
As you look at the diagram, all transition probabilities were defined. Transition probability generally are denoted as $a_{ij}$ which can be interpreted as the probability of the system to transition from
state <code>i</code> to state <code>j</code>.</p>
<p>$$
a_{ij} = P(s(t+1) = j | s(t) = i)
$$</p>
<p><img loading=lazy src=/posts/images/introduction-to-hidden-markov-model/3_transitions_diagram.png alt="Markov Model Transition Probabilities">
</p>
<p>For an example presented above, the transition probability from Sun to Cloud is defined as $a_{12}$. Note that the transition might happen to the same state also. If we have sun in two consecutive days
then the transition probability from sun to sun at the next time step will be $a_{11}$.</p>
<p>Generally, the transition probabilities are defined using a <code>(M x M)</code> matrix, known as <strong>Transition Probability Matrix</strong>. For our example, the Transition Probability Matrix would be defined as:</p>
<p>$$
A = \begin{bmatrix}a_{11} & a_{12} & a_{13} \\\ a_{21} & a_{22} & a_{23} \\\ a_{31} & a_{32} & a_{33} \end{bmatrix}
$$</p>
<p>Nice property - when the system transitions to another state, the sum of all transition probabilities given the current state should be equal to 1, so for example $a_{11} + a_{12} + a_{13} = 1$.</p>
<p>In general:</p>
<p>$$
\sum_{j=1}^{M} a_{ij} = 1 \; \; \; \forall i
$$</p>
<h2 id=initial-probability-distribution>Initial probability distribution<a hidden class=anchor aria-hidden=true href=#initial-probability-distribution>#</a></h2>
<p>The system has to start from one state. The initial state of Markov Model (when time step <code>t = 0</code>) is denoted as $\pi$ - N dimensional row vector.
All the probabilities must sum to 1, so $ \sum_{i=1}^{N} \pi_i = 1 \; \; \; \forall i$.</p>
<p>During implementation, we can assume the initial probability distribution is uniform. In our weather example, we could define initial state as
$ \pi = [ \frac{1}{3} \frac{1}{3} \frac{1}{3}]$.</p>
<p>In some cases we may have $\pi_i = 0$, if particular state can&rsquo;t be an initial one.</p>
<h2 id=markov-chain>Markov Chain<a hidden class=anchor aria-hidden=true href=#markov-chain>#</a></h2>
<p>When the system is fully observable and autonomous, it&rsquo;s called a <code>Markov Chain</code>.
What we&rsquo;ve learned so far is a great example.</p>
<p>We can conclude that Markov Chain consists of the following parameters:</p>
<ul>
<li>A set of <code>N states</code></li>
<li>A <code>transition probability</code> matrix A</li>
<li>An <code>initial probability distribution</code> $\pi$</li>
</ul>
<h2 id=absorbing-state>Absorbing state<a hidden class=anchor aria-hidden=true href=#absorbing-state>#</a></h2>
<p>As the name suggests, when transition probabilities from one state to all the other except for itself are 0, then it&rsquo;s called an Absorbing State.
When the system enters into that state, it never leaves.</p>
<h1 id=hidden-markov-model>Hidden Markov Model<a hidden class=anchor aria-hidden=true href=#hidden-markov-model>#</a></h1>
<p>In Hidden Markov Model the state of the system will be hidden (unknown), however at every time step <code>t</code> the system in state <code>s(t)</code> will
emit an observable symbol <code>v(t)</code>.</p>
<p>The idea is presented on a graph below:</p>
<p><img loading=lazy src=/posts/images/introduction-to-hidden-markov-model/2_transitions_hmm.png alt="Markov Model Transition Probabilities">
</p>
<p>In our initial example of dishonest casino, the die rolled (fair or unfair) is hidden. However every time the die is rolled, we know the outcome
(number between 1 and 6) - this is the symbol we observe.</p>
<p><strong>Summary:</strong></p>
<ul>
<li>We can define a particular sequence of visible states/symbols as $V^T = { v(1), v(2) … v(T) }$</li>
<li>Then define model $\theta$, so for every state <code>s(t)</code> we have a probability of emitting a particular visible state $v_k(t)$</li>
<li>Since we have access to the visible states only (<code>s(t)'s</code> are unobservable), a model is called a <code>Hidden Markov Model</code></li>
</ul>
<h2 id=emission-probability>Emission probability<a hidden class=anchor aria-hidden=true href=#emission-probability>#</a></h2>
<p>We can redefine our previous example. Assume based on the weather of any day the mood of a person changes from happy to sad.
Also assume the person is at a remote place and we don&rsquo;t know how the weather is there. We can only know the mood of the person.
In this scenario, the weather is the <code>hidden state</code> in the model and the mood (happy/sad) is the <code>visible state</code>. So we can use the HMM to predict
the weather, knowing the mood of the person.</p>
<p>The graphical presentation of transition mechanism looks like that:</p>
<p><img loading=lazy src=/posts/images/introduction-to-hidden-markov-model/4_transitions_diagram_hmm.png alt="Markov Model Transition Probabilities">
</p>
<p>The observable symbols are ${ v_1 , v_2 }$ - from each hidden state one of them must be emitted.
The probability of emitting a symbol is known as <code>emission probability</code> - defined by us as $b_{jk}$. Mathematically,
it is the probability of emitting symbol <code>k</code> given state <code>j</code>:</p>
<p>$$
b_{jk} = p(v_k(t) | s_j(t) )
$$</p>
<p>Emission probabilities are also defined using <code>N x M</code> matrix, called <code>Emission Probability Matrix</code>:</p>
<p>$$
B = \begin{bmatrix}
b_{11} & b_{12} \\\
b_{21} & b_{22} \\\<br>
b_{31} & b_{32}
\end{bmatrix}
$$</p>
<p>The Emission Probabilities, same as Transition Probabilities, sum to 1.</p>
<p>$$
\sum_{k=1}^{C} b_{jk} = 1, \; \; \; \forall j
$$</p>
<p>We have the attributes and properties of our Hidden Markov Model defined. Few more obstacles are waiting for us before reaching our ultimate goal for a new tool - prediction.</p>
<h1 id=central-issues-with-hidden-markov-model>Central issues with Hidden Markov Model<a hidden class=anchor aria-hidden=true href=#central-issues-with-hidden-markov-model>#</a></h1>
<h2 id=evaluation-problems>Evaluation problems<a hidden class=anchor aria-hidden=true href=#evaluation-problems>#</a></h2>
<p>We can define the model $\theta$ as following:</p>
<p>$$
\theta \rightarrow s, v, a_{ij},b_{jk}
$$</p>
<p>Given the model ($\theta$) and sequence of visible states ($V^T$), we need to determine the probability that a particular sequence
of visible states was generated from the model.</p>
<p>There could be many models ${ \theta_1, \theta_2 … \theta_n }$. We need to find $p(V^T | \theta_i)$ then use Bayes Rule to correctly
classify the sequence $V^T$.</p>
<p>$$
p(\theta | V^T ) = \frac{p(V^T | \theta) p(\theta)}{p(V^T)}
$$</p>
<h2 id=learning-problems>Learning problems<a hidden class=anchor aria-hidden=true href=#learning-problems>#</a></h2>
<p>In general HMM is unsupervised learning process - number of different visible symbol types is known (like happy/sad), but the number
of hidden states is not. We can always try out different options however this may result in significant amount of computation.
Picking the number of hidden states can be a more or less educated guess.</p>
<p>Once we have the high level structure (number of hidden and visible states) of the model, we need to estimate the transition ($a_{ij}$) and
emission ($b_{jk}$) probabilities using the training sequence of visible states - that&rsquo;s known as a learning problem.</p>
<p>We&rsquo;ll be using the evaluation problem to solve the learning problem, so understanding the former is necessary. The transition
and emission probabilities will be estimated with use of the <code>Expectation Maximization</code> algorithm. Entire learning problem is known as <code>Forward-Backward Algorithm</code>
or <code>Baum-Welch Algorithm</code>.</p>
<h2 id=decoding-problem>Decoding problem<a hidden class=anchor aria-hidden=true href=#decoding-problem>#</a></h2>
<p>Once we have the estimates for transition ($a_{ij}$) and emission ($b_{jk}$) probabilities, we can use the model ($\theta$) to predict the
hidden states ($W^T$) which generated the visible sequence ($V^T$). The decoding problem is known as <code>Viterbi Algorithm</code>.</p>
<h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1>
<p>The introduction gives some intuition about the Hidden Markov Models. To implement the algorithm from scratch, we have to go through
three problems defined above as the next steps.</p>
</div>
<footer class=post-footer>
<nav class=paginav>
<a class=prev href=/posts/forward-and-backward-algorithm-in-hidden-markov-model/>
<span class=title>« Prev Page</span>
<br>
<span>Forward and Backward Algorithm in Hidden Markov Model</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Hidden Markov Model on twitter" href="https://twitter.com/intent/tweet/?text=Introduction%20to%20Hidden%20Markov%20Model&url=%2fposts%2fintroduction-to-hidden-markov-model%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Hidden Markov Model on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=%2fposts%2fintroduction-to-hidden-markov-model%2f&title=Introduction%20to%20Hidden%20Markov%20Model&summary=Introduction%20to%20Hidden%20Markov%20Model&source=%2fposts%2fintroduction-to-hidden-markov-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Hidden Markov Model on reddit" href="https://reddit.com/submit?url=%2fposts%2fintroduction-to-hidden-markov-model%2f&title=Introduction%20to%20Hidden%20Markov%20Model"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Hidden Markov Model on facebook" href="https://facebook.com/sharer/sharer.php?u=%2fposts%2fintroduction-to-hidden-markov-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Hidden Markov Model on whatsapp" href="https://api.whatsapp.com/send?text=Introduction%20to%20Hidden%20Markov%20Model%20-%20%2fposts%2fintroduction-to-hidden-markov-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Hidden Markov Model on telegram" href="https://telegram.me/share/url?text=Introduction%20to%20Hidden%20Markov%20Model&url=%2fposts%2fintroduction-to-hidden-markov-model%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href>Marcin Halupka</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+="has-jax"})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</body>
</html>