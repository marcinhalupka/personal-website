<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Forward and Backward Algorithm in Hidden Markov Model | Marcin Halupka</title>
<meta name=keywords content>
<meta name=description content="Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (Evaluation, Learning and Decoding). In this post we&rsquo;ll deep dive into the Evaluation Problem. Starting from mathematical understanding, finishing on Python and R implementations.
Quick recap Hidden Markov Model is a Markov Chain which is mainly used in problems with temporal sequence of the data. Markov Model explaimns that the next step depends only on the previous step in a temporal sequence.">
<meta name=author content="Marcin Halupka">
<link rel=canonical href=/posts/forward-and-backward-algorithm-in-hidden-markov-model/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css integrity="sha256-b2AFbUTT9+tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=16x16 href=%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=%3Clink%20/%20abs%20url%3E>
<link rel=apple-touch-icon href=%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="Forward and Backward Algorithm in Hidden Markov Model">
<meta property="og:description" content="Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (Evaluation, Learning and Decoding). In this post we&rsquo;ll deep dive into the Evaluation Problem. Starting from mathematical understanding, finishing on Python and R implementations.
Quick recap Hidden Markov Model is a Markov Chain which is mainly used in problems with temporal sequence of the data. Markov Model explaimns that the next step depends only on the previous step in a temporal sequence.">
<meta property="og:type" content="article">
<meta property="og:url" content="/posts/forward-and-backward-algorithm-in-hidden-markov-model/"><meta property="og:image" content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-10-26T00:09:35+02:00">
<meta property="article:modified_time" content="2021-10-26T00:09:35+02:00"><meta property="og:site_name" content="ExampleSite">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="Forward and Backward Algorithm in Hidden Markov Model">
<meta name=twitter:description content="Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (Evaluation, Learning and Decoding). In this post we&rsquo;ll deep dive into the Evaluation Problem. Starting from mathematical understanding, finishing on Python and R implementations.
Quick recap Hidden Markov Model is a Markov Chain which is mainly used in problems with temporal sequence of the data. Markov Model explaimns that the next step depends only on the previous step in a temporal sequence.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"/posts/"},{"@type":"ListItem","position":3,"name":"Forward and Backward Algorithm in Hidden Markov Model","item":"/posts/forward-and-backward-algorithm-in-hidden-markov-model/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Forward and Backward Algorithm in Hidden Markov Model","name":"Forward and Backward Algorithm in Hidden Markov Model","description":"Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (Evaluation, Learning and Decoding). In this post we\u0026rsquo;ll deep dive into the Evaluation Problem. Starting from mathematical understanding, finishing on Python and R implementations.\nQuick recap Hidden Markov Model is a Markov Chain which is mainly used in problems with temporal sequence of the data. Markov Model explaimns that the next step depends only on the previous step in a temporal sequence.","keywords":[],"articleBody":"Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (Evaluation, Learning and Decoding). In this post we’ll deep dive into the Evaluation Problem. Starting from mathematical understanding, finishing on Python and R implementations.\nQuick recap Hidden Markov Model is a Markov Chain which is mainly used in problems with temporal sequence of the data. Markov Model explaimns that the next step depends only on the previous step in a temporal sequence. In Hidden Markov Model the state of the system is hidden however each state emits a visible symbol at every time step. HMM can work with both discrete and continuous sequence of the data but we’ll focus on the former.\nBasic structure of HMM Hidden Markov Model ($\\theta$) has the following parameters:\n Set of $M$ Hidden States ($S^M$) A Transaction Probability Matrix ($A$) A sequence of T observations ($V^T$) An Emission Probability Matrix ($B$) An initial probability distribution ($\\pi$)  Evaluation problem As we have seen before, the Evaluation Problem can be stated as follows:\n$$ \\displaylines{ \\text{Given } \\theta, V_T \\rightarrow \\text{Estimate } P(V_T|\\theta) \\\\\\\n\\text{Where } \\theta \\rightarrow s, v, a_{ij},b_{jk} } $$\nSolution  First we need to find all possible sequences of the states $S^M$ where M is the number of Hidden States Then from all those sequences of $S^M$, find the probability of which sequence generated the visible sequence of symbols $V^T$ Mathematically $P(V_T|\\theta)$ can be estimated as $p(V^T|\\theta)= \\sum_{r=1}^{R} p(V^T|S_r^T)p(S_r^T), \\, \\, \\text{where }S_r^T = { s_1(1), s_2(2)… s_r(T)}$\nand $R$ - maximum number of possivle sequences of the hidden state.  If there are $M$ possible hidden states, then $R = M^T$.\nIn order to compute the probability of the model generated by the particular sequence of T visible symbols $V^T$, we should take each conceivable sequence of hidden states, calculate probability that they have produced $V^T$ and then add up these probabilities.\nHow to proof the solution is valid? Let’s present it in a different way.\nComing back to one of our previous examples, the diagram below presents a sequence of 3 states. The transition probabilities between hidden states are unknown.\nIn case in the example above we already know the sequence of the hidden states (sun, sun, cloud) which generated the 3 visible symbols (happy, sad, happy).\nCalculation of the probability of the visible symbols given the hidden states is pretty straightforward.\n$$ P(\\text{happy, sad, happy} | \\text{sun, sun, rain}) = P(\\text{happy} | \\text{sun}) \\times P(\\text{sad} | \\text{sun}) \\times P(\\text{happy} | \\text{rain}) $$\nMathematically probability of $V^T$ given $S^T$ can be written as:\n$$ p(V^T|S_r^T)=\\prod_{t=1}^{T} p(v(t) | s(t)) $$\nUnfortunately we don’t know the specific sequence of hidden states that generated the visible symbols. We need to compute the probability of mood changes (happy, sad, happy) by summing over all possible weather sequences weighted by their probability (transition probability).\nWe can calculate the joint probability of the sequence of visible symbol $V^T$ generated by a specific sequence of hidden state $S^T$ as:\n$$ \\displaylines{ P(\\text{happy, sad, happy} | \\text{sun, sun, rain}) = P(\\text{sun} | \\text{initial state}) \\times P(\\text{sun} | \\text{sun}) \\times \\\\\\\n\\times P(\\text{rain} | \\text{sun}) \\times P(\\text{happy} | \\text{sun}) \\times P(\\text{sad} | \\text{sun}) \\times P(\\text{happy} | \\text{rain}) } $$\nIn general:\n$$ P(V^T,S^T)=P(V^T | S^T)P(S^T) $$\nSince we’re using First-Order Markov Model, we can say that the probability of a sequence of T hidden states is the multiplication of the probability of each transition:\n$$ P(S^T)=\\prod_{t=1}^{T} P(s(t) | s(t-1)) $$\nSo the joint probability can be written as:\n$$\\begin{eqnarray} P(V^T,S^T) \u0026=\u0026 P(V^T | S^T)P(S^T) \\\\\\\n\u0026=\u0026 \\prod_{t=1}^{T} P(v(t) | s(t)) \\prod_{t=1}^{T} P(s(t) | s(t-1)) \\end{eqnarray}$$\nWe are getting close to our original equation, just one more step is left. The above equation is for a specific sequence of hidden states that we thought could have generated the visible sequence of symbols. We should compute the probability of all the different possible sequences of hidden states by summing over all the joint probabilities of $V^T$ and $S^T$.\nIn out example, we have a sequence of 3 visible states and 2 hidden states that could emit them. There can be $2^3 = 8$ possible sequences.\nThe generalized equation:\n$$ \\begin{eqnarray} P(V^T|\\theta) \u0026=\u0026 \\sum_{\\text{all seq of S}} P(V^T, S^T) \\\\\\\n\u0026=\u0026 \\sum_{\\text{all seq of S}} P(V^T | S^T)P(S^T) \\\\\\\n\u0026=\u0026 \\sum_{r=1}^R \\prod_{t=1}^{T} P(v(t) | s(t)) \\prod_{t=1}^{T} P(s(t) | s(t-1)) \\\\\\\n\u0026=\u0026 \\sum_{r=1}^R \\prod_{t=1}^{T} P(v(t) | s(t)) P(s(t) | s(t-1)) \\end{eqnarray} $$\nAs previously, $R$ - number of possible sequences of the hidden states.\nA bad news is that the computation complexity of that approach is $O(N^T T)$ which makes it not really practical. We need to find easy to compute alternative and a recursive dynamic programming is the one that can save us from the exponential computation. There are two such algorithms, Forward Algorithm and Backward Algorithm.\nForward algorithm As the name suggests, we want to use the computed probability on current time step to compute the probability of the next time step. The computational complexity is far more efficient than previous $O(N^T T)$.\nTo do this, we need to find the answer the following question - given a sequence of Visible states $V^T$ what is the probability that the Hidden Markov Model will be in a particular hidden state $s$ at a particular time step $t$?\nThe question rewritten as mathematical formula:\n$$ \\alpha_j(t) = p(v(1)…v(t),s(t)= j) = ? $$\nWe’ll start with deriving the equation using probability and then solve it again using trellis diagram. If the former is too dificult, skip it and go straight to the diagram.\nSolution using probabilities Before deriving a general equation, let’s start with calculations for the small lengths of the sequence.\nWhen t = 1\n$$ \\begin{eqnarray} \\alpha_j(1) \u0026=\u0026 p(v_k(1),s(1)= j) \\\\\\\n\u0026=\u0026 p(v_k(1)|s(1)=j)p(s(1)=j) \\\\\\\n\u0026=\u0026 \\pi_j p(v_k(1)|s(1)=j) \\\\\\\n\u0026=\u0026 \\pi_j b_{jk} \\\\\\\n\\text{where } \\pi \u0026=\u0026 \\text{ initial distribution, } \\\\\\\nb_{jkv(1)} \u0026=\u0026 \\text{ emission probability at } t = 1 \\end{eqnarray} $$\nWhen t = 2\nWe have the solution for t=1. Now we want to rewrite the same for t=2 and come up with equation with $\\alpha_j(1)$ as a part of it so we can use recursion.\n$$ \\begin{eqnarray} \\alpha_j(2) \u0026=\u0026 p \\Big( v_k(1),v_k(2),s(2)= j \\Big) \\\\\\\n\u0026=\u0026 {\\sum_{i=1}^M} p \\Big( v_k(1),v_k(2),{s(1)= i}, s(2)= j \\Big) \\\\\\ \u0026=\u0026 \\sum_{i=1}^M p \\Big( v_k(2) | s(2)= j, v_k(1),s(1)= i \\Big) p \\Big( v_k(1),s(2),s(1)= i \\Big)\\\\\\\n\u0026=\u0026 \\sum_{i=1}^M p \\Big( v_k(2) | s(2)= j, {v_k(1), s(1)= i} \\Big) p \\Big( s(2) | {v_k(1),}s(1)= i \\Big) p \\Big(v_k(1),s(1)= i \\Big) \\\\\\\n\u0026=\u0026 \\sum_{i=1}^M p \\Big( v_k(2) | s(2)= j \\Big) p \\Big(s(2) | s(1)= i \\Big) p \\Big(v_k(1),s(1)= i \\Big)\\\\\\\n\u0026=\u0026 {p \\Big( v_k(2) | s(2)= j \\Big) }\\sum_{i=1}^M p \\Big( s(2) | s(1)= i \\Big) {p \\Big( v_k(1),s(1)= i \\Big)} \\\\\\\n\u0026=\u0026 {b_{jk v(2)}} \\sum_{i=1}^M a_{i2} {\\alpha_i(1)}\\\\\\\n\\text{where } a_{i2} \u0026=\u0026 \\text{ transition probability } \\\\\\\nb_{jk v(2)} \u0026=\u0026 \\text{ emission probability at } t=2 \\\\\\\n\\alpha_i(1) \u0026=\u0026 \\text{ forward probability at } t=1 \\end{eqnarray} $$\nGeneralized equation Placeholder for text.\nIntuition using Trellis Placeholder for text.\nImplementation of Forward algorithm Placeholder for text.\nBackward algorithm Placeholder for text.\nDerivation of Backward algorithm Placeholder for text.\nIntuition using Trellis Placeholder for text.\nImplementation of Backward algorithm Placeholder for text.\nConclusion Placeholder for text.\n","wordCount":"1207","inLanguage":"en","datePublished":"2021-10-26T00:09:35+02:00","dateModified":"2021-10-26T00:09:35+02:00","author":{"@type":"Person","name":"Marcin Halupka"},"mainEntityOfPage":{"@type":"WebPage","@id":"/posts/forward-and-backward-algorithm-in-hidden-markov-model/"},"publisher":{"@type":"Organization","name":"Marcin Halupka","logo":{"@type":"ImageObject","url":"%3Clink%20/%20abs%20url%3E"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href accesskey=h title="Marcin Halupka (Alt + H)">Marcin Halupka</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=/posts/ title=Posts>
<span>Posts</span>
</a>
</li>
<li>
<a href=/projects/ title=Projects>
<span>Projects</span>
</a>
</li>
<li>
<a href=/about/ title="About me">
<span>About me</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href>Home</a>&nbsp;»&nbsp;<a href=/posts/>Posts</a></div>
<h1 class=post-title>
Forward and Backward Algorithm in Hidden Markov Model
</h1>
<div class=post-meta>October 26, 2021&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Marcin Halupka&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/forward-and-backward-algorithm-in-hidden-markov-model.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header> <div class=toc>
<details open>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#quick-recap aria-label="Quick recap">Quick recap</a><ul>
<li>
<a href=#basic-structure-of-hmm aria-label="Basic structure of HMM">Basic structure of HMM</a></li></ul>
</li>
<li>
<a href=#evaluation-problem aria-label="Evaluation problem">Evaluation problem</a><ul>
<li>
<a href=#solution aria-label=Solution>Solution</a></li>
<li>
<a href=#how-to-proof-the-solution-is-valid aria-label="How to proof the solution is valid?">How to proof the solution is valid?</a></li></ul>
</li>
<li>
<a href=#forward-algorithm aria-label="Forward algorithm">Forward algorithm</a><ul>
<li>
<a href=#solution-using-probabilities aria-label="Solution using probabilities">Solution using probabilities</a></li>
<li>
<a href=#generalized-equation aria-label="Generalized equation">Generalized equation</a></li>
<li>
<a href=#intuition-using-trellis aria-label="Intuition using Trellis">Intuition using Trellis</a></li>
<li>
<a href=#implementation-of-forward-algorithm aria-label="Implementation of Forward algorithm">Implementation of Forward algorithm</a></li></ul>
</li>
<li>
<a href=#backward-algorithm aria-label="Backward algorithm">Backward algorithm</a><ul>
<li>
<a href=#derivation-of-backward-algorithm aria-label="Derivation of Backward algorithm">Derivation of Backward algorithm</a></li>
<li>
<a href=#intuition-using-trellis-1 aria-label="Intuition using Trellis">Intuition using Trellis</a></li>
<li>
<a href=#implementation-of-backward-algorithm aria-label="Implementation of Backward algorithm">Implementation of Backward algorithm</a></li></ul>
</li>
<li>
<a href=#conclusion aria-label=Conclusion>Conclusion</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p>Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (<code>Evaluation</code>, <code>Learning</code> and <code>Decoding</code>).
In this post we&rsquo;ll deep dive into the <strong>Evaluation Problem</strong>. Starting from mathematical understanding, finishing on Python and R implementations.</p>
<h1 id=quick-recap>Quick recap<a hidden class=anchor aria-hidden=true href=#quick-recap>#</a></h1>
<p>Hidden Markov Model is a <strong>Markov Chain</strong> which is mainly used in problems with <strong>temporal sequence</strong> of the data. Markov Model explaimns that the next step depends only on the previous step in
a temporal sequence. In Hidden Markov Model the state of the system is <code>hidden</code> however each state emits a visible symbol at every time step. HMM can work with both discrete and continuous sequence of the data
but we&rsquo;ll focus on the former.</p>
<h2 id=basic-structure-of-hmm>Basic structure of HMM<a hidden class=anchor aria-hidden=true href=#basic-structure-of-hmm>#</a></h2>
<p>Hidden Markov Model ($\theta$) has the following parameters:</p>
<ul>
<li>Set of $M$ Hidden States ($S^M$)</li>
<li>A Transaction Probability Matrix ($A$)</li>
<li>A sequence of T observations ($V^T$)</li>
<li>An Emission Probability Matrix ($B$)</li>
<li>An initial probability distribution ($\pi$)</li>
</ul>
<h1 id=evaluation-problem>Evaluation problem<a hidden class=anchor aria-hidden=true href=#evaluation-problem>#</a></h1>
<p>As we have seen before, the Evaluation Problem can be stated as follows:</p>
<p>$$
\displaylines{
\text{Given } \theta, V_T \rightarrow \text{Estimate } P(V_T|\theta) \\\<br>
\text{Where } \theta \rightarrow s, v, a_{ij},b_{jk}
}
$$</p>
<h2 id=solution>Solution<a hidden class=anchor aria-hidden=true href=#solution>#</a></h2>
<ol>
<li>First we need to find all possible sequences of the states $S^M$ where <code>M</code> is the number of Hidden States</li>
<li>Then from all those sequences of $S^M$, find the probability of which sequence generated the visible sequence of symbols $V^T$</li>
<li>Mathematically $P(V_T|\theta)$ can be estimated as $p(V^T|\theta)= \sum_{r=1}^{R} p(V^T|S_r^T)p(S_r^T), \, \, \text{where }S_r^T = { s_1(1), s_2(2)… s_r(T)}$<br>
and $R$ - maximum number of possivle sequences of the hidden state.</li>
</ol>
<p>If there are $M$ possible hidden states, then $R = M^T$.</p>
<p>In order to compute the probability of the model generated by the <em>particular</em> sequence of <code>T</code> visible symbols $V^T$, we should take each conceivable sequence of hidden states, calculate probability that they
have produced $V^T$ and then add up these probabilities.</p>
<h2 id=how-to-proof-the-solution-is-valid>How to proof the solution is valid?<a hidden class=anchor aria-hidden=true href=#how-to-proof-the-solution-is-valid>#</a></h2>
<p>Let&rsquo;s present it in a different way.</p>
<p>Coming back to one of our previous examples, the diagram below presents a sequence of 3 states. The transition probabilities between hidden states are unknown.</p>
<p><img loading=lazy src=/posts/images/3/transitions-visible.png alt="Transitions between hidden layers with unknown probabilities">
</p>
<p>In case in the example above we already know the sequence of the hidden states (<code>sun</code>, <code>sun</code>, <code>cloud</code>) which generated the 3 visible symbols (<code>happy</code>, <code>sad</code>, <code>happy</code>).</p>
<p>Calculation of the probability of the visible symbols given the hidden states is pretty straightforward.</p>
<p>$$
P(\text{happy, sad, happy} | \text{sun, sun, rain}) = P(\text{happy} | \text{sun}) \times P(\text{sad} | \text{sun}) \times P(\text{happy} | \text{rain})
$$</p>
<p>Mathematically probability of $V^T$ given $S^T$ can be written as:</p>
<p>$$
p(V^T|S_r^T)=\prod_{t=1}^{T} p(v(t) | s(t))
$$</p>
<p>Unfortunately we <strong>don&rsquo;t know the specific sequence of hidden states</strong> that generated the visible symbols. We need to compute the probability of mood changes
(<code>happy</code>, <code>sad</code>, <code>happy</code>) by <strong>summing over all possible weather sequences</strong> weighted by their probability (transition probability).</p>
<p><img loading=lazy src=/posts/images/3/transitions-hidden.png alt="Transitions between hidden layers known with probabilities">
</p>
<p>We can calculate the <strong>joint probability</strong> of the sequence of visible symbol $V^T$ generated by a specific sequence of hidden state $S^T$ as:</p>
<p>$$
\displaylines{
P(\text{happy, sad, happy} | \text{sun, sun, rain}) = P(\text{sun} | \text{initial state}) \times P(\text{sun} | \text{sun}) \times \\\<br>
\times P(\text{rain} | \text{sun}) \times P(\text{happy} | \text{sun}) \times P(\text{sad} | \text{sun}) \times P(\text{happy} | \text{rain})
}
$$</p>
<p>In general:</p>
<p>$$
P(V^T,S^T)=P(V^T | S^T)P(S^T)
$$</p>
<p>Since we&rsquo;re using <strong>First-Order Markov Model</strong>, we can say that the probability of a sequence of <code>T</code> hidden states is the multiplication of the probability of each transition:</p>
<p>$$
P(S^T)=\prod_{t=1}^{T} P(s(t) | s(t-1))
$$</p>
<p>So the joint probability can be written as:</p>
<p>$$\begin{eqnarray}
P(V^T,S^T) &=& P(V^T | S^T)P(S^T) \\\<br>
&=& \prod_{t=1}^{T} P(v(t) | s(t)) \prod_{t=1}^{T} P(s(t) | s(t-1))
\end{eqnarray}$$</p>
<p>We are getting close to our original equation, just one more step is left. The above equation is for a <strong>specific sequence</strong> of hidden states that we thought could have generated
the visible sequence of symbols. We should compute the probability of <strong>all</strong> the different possible sequences of hidden states by <strong>summing over all the joint probabilities</strong> of
$V^T$ and $S^T$.</p>
<p>In out example, we have a sequence of 3 visible states and 2 hidden states that could emit them. There can be $2^3 = 8$ possible sequences.</p>
<p>The generalized equation:</p>
<p>$$
\begin{eqnarray}
P(V^T|\theta) &=& \sum_{\text{all seq of S}} P(V^T, S^T) \\\<br>
&=& \sum_{\text{all seq of S}} P(V^T | S^T)P(S^T) \\\<br>
&=& \sum_{r=1}^R \prod_{t=1}^{T} P(v(t) | s(t)) \prod_{t=1}^{T} P(s(t) | s(t-1)) \\\<br>
&=& \sum_{r=1}^R \prod_{t=1}^{T} P(v(t) | s(t)) P(s(t) | s(t-1))
\end{eqnarray}
$$</p>
<p>As previously, $R$ - number of possible sequences of the hidden states.</p>
<p>A bad news is that the <strong>computation complexity</strong> of that approach is $O(N^T T)$ which makes it not really practical. We need to find easy to compute alternative and a <strong>recursive dynamic programming</strong>
is the one that can save us from the exponential computation. There are two such algorithms, <code>Forward Algorithm</code> and <code>Backward Algorithm</code>.</p>
<h1 id=forward-algorithm>Forward algorithm<a hidden class=anchor aria-hidden=true href=#forward-algorithm>#</a></h1>
<p>As the name suggests, we want to use the computed probability on <strong>current time step</strong> to compute the probability of the <strong>next time step</strong>. The computational complexity is far more efficient than previous $O(N^T T)$.</p>
<p>To do this, we need to find the answer the following question - given a sequence of Visible states $V^T$ what is the probability that the Hidden Markov Model will be in a particular hidden state $s$ at a particular
time step $t$?</p>
<p>The question rewritten as mathematical formula:</p>
<p>$$
\alpha_j(t) = p(v(1)…v(t),s(t)= j) = ?
$$</p>
<p>We&rsquo;ll start with deriving the equation using probability and then solve it again using <code>trellis diagram</code>. If the former is too dificult, skip it and go straight to the diagram.</p>
<h2 id=solution-using-probabilities>Solution using probabilities<a hidden class=anchor aria-hidden=true href=#solution-using-probabilities>#</a></h2>
<p>Before deriving a general equation, let&rsquo;s start with calculations for the small lengths of the sequence.</p>
<p><strong>When <code>t = 1</code></strong></p>
<p>$$
\begin{eqnarray}
\alpha_j(1) &=& p(v_k(1),s(1)= j) \\\<br>
&=& p(v_k(1)|s(1)=j)p(s(1)=j) \\\<br>
&=& \pi_j p(v_k(1)|s(1)=j) \\\<br>
&=& \pi_j b_{jk} \\\<br>
\text{where } \pi &=& \text{ initial distribution, } \\\<br>
b_{jkv(1)} &=& \text{ emission probability at } t = 1
\end{eqnarray}
$$</p>
<p><strong>When <code>t = 2</code></strong></p>
<p>We have the solution for <code>t=1</code>. Now we want to rewrite the same for <code>t=2</code> and come up with equation with $\alpha_j(1)$ as a part of it so we can use <code>recursion</code>.</p>
<p>$$
\begin{eqnarray}
\alpha_j(2) &=& p \Big( v_k(1),v_k(2),s(2)= j \Big) \\\<br>
&=& {\sum_{i=1}^M} p \Big( v_k(1),v_k(2),{s(1)= i}, s(2)= j \Big) \\\
&=& \sum_{i=1}^M p \Big( v_k(2) | s(2)= j, v_k(1),s(1)= i \Big) p \Big( v_k(1),s(2),s(1)= i \Big)\\\<br>
&=& \sum_{i=1}^M p \Big( v_k(2) | s(2)= j, {v_k(1), s(1)= i} \Big) p \Big( s(2) | {v_k(1),}s(1)= i \Big) p \Big(v_k(1),s(1)= i \Big) \\\<br>
&=& \sum_{i=1}^M p \Big( v_k(2) | s(2)= j \Big) p \Big(s(2) | s(1)= i \Big) p \Big(v_k(1),s(1)= i \Big)\\\<br>
&=& {p \Big( v_k(2) | s(2)= j \Big) }\sum_{i=1}^M p \Big( s(2) | s(1)= i \Big) {p \Big( v_k(1),s(1)= i \Big)} \\\<br>
&=& {b_{jk v(2)}} \sum_{i=1}^M a_{i2} {\alpha_i(1)}\\\<br>
\text{where } a_{i2} &=& \text{ transition probability } \\\<br>
b_{jk v(2)} &=& \text{ emission probability at } t=2 \\\<br>
\alpha_i(1) &=& \text{ forward probability at } t=1
\end{eqnarray}
$$</p>
<h2 id=generalized-equation>Generalized equation<a hidden class=anchor aria-hidden=true href=#generalized-equation>#</a></h2>
<p>Placeholder for text.</p>
<h2 id=intuition-using-trellis>Intuition using Trellis<a hidden class=anchor aria-hidden=true href=#intuition-using-trellis>#</a></h2>
<p>Placeholder for text.</p>
<h2 id=implementation-of-forward-algorithm>Implementation of Forward algorithm<a hidden class=anchor aria-hidden=true href=#implementation-of-forward-algorithm>#</a></h2>
<p>Placeholder for text.</p>
<h1 id=backward-algorithm>Backward algorithm<a hidden class=anchor aria-hidden=true href=#backward-algorithm>#</a></h1>
<p>Placeholder for text.</p>
<h2 id=derivation-of-backward-algorithm>Derivation of Backward algorithm<a hidden class=anchor aria-hidden=true href=#derivation-of-backward-algorithm>#</a></h2>
<p>Placeholder for text.</p>
<h2 id=intuition-using-trellis-1>Intuition using Trellis<a hidden class=anchor aria-hidden=true href=#intuition-using-trellis-1>#</a></h2>
<p>Placeholder for text.</p>
<h2 id=implementation-of-backward-algorithm>Implementation of Backward algorithm<a hidden class=anchor aria-hidden=true href=#implementation-of-backward-algorithm>#</a></h2>
<p>Placeholder for text.</p>
<h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1>
<p>Placeholder for text.</p>
</div>
<footer class=post-footer>
<nav class=paginav>
<a class=prev href=/posts/introduction-to-time-series-forecasting/>
<span class=title>« Prev Page</span>
<br>
<span>Introduction to Time Series Forecasting</span>
</a>
<a class=next href=/posts/introduction-to-hidden-markov-model/>
<span class=title>Next Page »</span>
<br>
<span>Introduction to Hidden Markov Model</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Forward and Backward Algorithm in Hidden Markov Model on twitter" href="https://twitter.com/intent/tweet/?text=Forward%20and%20Backward%20Algorithm%20in%20Hidden%20Markov%20Model&url=%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Forward and Backward Algorithm in Hidden Markov Model on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f&title=Forward%20and%20Backward%20Algorithm%20in%20Hidden%20Markov%20Model&summary=Forward%20and%20Backward%20Algorithm%20in%20Hidden%20Markov%20Model&source=%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Forward and Backward Algorithm in Hidden Markov Model on reddit" href="https://reddit.com/submit?url=%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f&title=Forward%20and%20Backward%20Algorithm%20in%20Hidden%20Markov%20Model"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Forward and Backward Algorithm in Hidden Markov Model on facebook" href="https://facebook.com/sharer/sharer.php?u=%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Forward and Backward Algorithm in Hidden Markov Model on whatsapp" href="https://api.whatsapp.com/send?text=Forward%20and%20Backward%20Algorithm%20in%20Hidden%20Markov%20Model%20-%20%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Forward and Backward Algorithm in Hidden Markov Model on telegram" href="https://telegram.me/share/url?text=Forward%20and%20Backward%20Algorithm%20in%20Hidden%20Markov%20Model&url=%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href>Marcin Halupka</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+="has-jax"})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</body>
</html>