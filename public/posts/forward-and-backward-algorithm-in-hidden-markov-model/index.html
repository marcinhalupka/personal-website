<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Forward and Backward Algorithm in Hidden Markov Model | Marcin Halupka</title>
<meta name=keywords content>
<meta name=description content="Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (Evaluation, Learning and Decoding). In this post we&rsquo;ll deep dive into the Evaluation Problem. Starting from mathematical understanding, finishing on Python and R implementations.
Quick recap Hidden Markov Model is a Markov Chain which is mainly used in problems with temporal sequence of the data. Markov Model explaimns that the next step depends only on the previous step in a temporal sequence.">
<meta name=author content="Marcin Halupka">
<link rel=canonical href=/posts/forward-and-backward-algorithm-in-hidden-markov-model/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css integrity="sha256-b2AFbUTT9+tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=16x16 href=%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=%3Clink%20/%20abs%20url%3E>
<link rel=apple-touch-icon href=%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="Forward and Backward Algorithm in Hidden Markov Model">
<meta property="og:description" content="Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (Evaluation, Learning and Decoding). In this post we&rsquo;ll deep dive into the Evaluation Problem. Starting from mathematical understanding, finishing on Python and R implementations.
Quick recap Hidden Markov Model is a Markov Chain which is mainly used in problems with temporal sequence of the data. Markov Model explaimns that the next step depends only on the previous step in a temporal sequence.">
<meta property="og:type" content="article">
<meta property="og:url" content="/posts/forward-and-backward-algorithm-in-hidden-markov-model/"><meta property="og:image" content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-10-26T00:09:35+02:00">
<meta property="article:modified_time" content="2021-10-26T00:09:35+02:00"><meta property="og:site_name" content="ExampleSite">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="Forward and Backward Algorithm in Hidden Markov Model">
<meta name=twitter:description content="Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (Evaluation, Learning and Decoding). In this post we&rsquo;ll deep dive into the Evaluation Problem. Starting from mathematical understanding, finishing on Python and R implementations.
Quick recap Hidden Markov Model is a Markov Chain which is mainly used in problems with temporal sequence of the data. Markov Model explaimns that the next step depends only on the previous step in a temporal sequence.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"/posts/"},{"@type":"ListItem","position":3,"name":"Forward and Backward Algorithm in Hidden Markov Model","item":"/posts/forward-and-backward-algorithm-in-hidden-markov-model/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Forward and Backward Algorithm in Hidden Markov Model","name":"Forward and Backward Algorithm in Hidden Markov Model","description":"Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (Evaluation, Learning and Decoding). In this post we\u0026rsquo;ll deep dive into the Evaluation Problem. Starting from mathematical understanding, finishing on Python and R implementations.\nQuick recap Hidden Markov Model is a Markov Chain which is mainly used in problems with temporal sequence of the data. Markov Model explaimns that the next step depends only on the previous step in a temporal sequence.","keywords":[],"articleBody":"Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (Evaluation, Learning and Decoding). In this post we’ll deep dive into the Evaluation Problem. Starting from mathematical understanding, finishing on Python and R implementations.\nQuick recap Hidden Markov Model is a Markov Chain which is mainly used in problems with temporal sequence of the data. Markov Model explaimns that the next step depends only on the previous step in a temporal sequence. In Hidden Markov Model the state of the system is hidden however each state emits a visible symbol at every time step. HMM can work with both discrete and continuous sequence of the data but we’ll focus on the former.\nBasic structure of HMM Hidden Markov Model ($\\theta$) has the following parameters:\n Set of $M$ Hidden States ($S^M$) A Transaction Probability Matrix ($A$) A sequence of T observations ($V^T$) An Emission Probability Matrix ($B$) An initial probability distribution ($\\pi$)  Evaluation problem As we have seen before, the Evaluation Problem can be stated as follows:\n$$ \\displaylines{ \\text{Given } \\theta, V_T \\rightarrow \\text{Estimate } P(V_T|\\theta) \\\\\\\n\\text{Where } \\theta \\rightarrow s, v, a_{ij},b_{jk} } $$\nSolution  First we need to find all possible sequences of the states $S^M$ where M is the number of Hidden States Then from all those sequences of $S^M$, find the probability of which sequence generated the visible sequence of symbols $V^T$ Mathematically $P(V_T|\\theta)$ can be estimated as $p(V^T|\\theta)= \\sum_{r=1}^{R} p(V^T|S_r^T)p(S_r^T), \\, \\, \\text{where }S_r^T = { s_1(1), s_2(2)… s_r(T)}$\nand $R$ - maximum number of possivle sequences of the hidden state.  If there are $M$ possible hidden states, then $R = M^T$.\nIn order to compute the probability of the model generated by the particular sequence of T visible symbols $V^T$, we should take each conceivable sequence of hidden states, calculate probability that they have produced $V^T$ and then add up these probabilities.\nHow to proof the solution is valid? Let’s present it in a different way.\nComing back to one of our previous examples, the diagram below presents a sequence of 3 states. The transition probabilities between hidden states are unknown.\nIn case in the example above we already know the sequence of the hidden states (sun, sun, cloud) which generated the 3 visible symbols (happy, sad, happy).\nCalculation of the probability of the visible symbols given the hidden states is pretty straightforward.\n$$ P(\\text{happy, sad, happy} | \\text{sun, sun, rain}) = P(\\text{happy} | \\text{sun}) \\times P(\\text{sad} | \\text{sun}) \\times P(\\text{happy} | \\text{rain}) $$\nMathematically probability of $V^T$ given $S^T$ can be written as:\n$$ p(V^T|S_r^T)=\\prod_{t=1}^{T} p(v(t) | s(t)) $$\nUnfortunately we don’t know the specific sequence of hidden states that generated the visible symbols. We need to compute the probability of mood changes (happy, sad, happy) by summing over all possible weather sequences weighted by their probability (transition probability).\nWe can calculate the joint probability of the sequence of visible symbol $V^T$ generated by a specific sequence of hidden state $S^T$ as:\n$$ \\displaylines{ P(\\text{happy, sad, happy} | \\text{sun, sun, rain}) = P(\\text{sun} | \\text{initial state}) \\times P(\\text{sun} | \\text{sun}) \\times \\\\\\\n\\times P(\\text{rain} | \\text{sun}) \\times P(\\text{happy} | \\text{sun}) \\times P(\\text{sad} | \\text{sun}) \\times P(\\text{happy} | \\text{rain}) } $$\nIn general:\n$$ P(V^T,S^T)=P(V^T | S^T)P(S^T) $$\nSince we’re using First-Order Markov Model, we can say that the probability of a sequence of T hidden states is the multiplication of the probability of each transition:\n$$ P(S^T)=\\prod_{t=1}^{T} P(s(t) | s(t-1)) $$\nSo the joint probability can be written as:\n$$\\begin{eqnarray} P(V^T,S^T) \u0026=\u0026 P(V^T | S^T)P(S^T) \\\\\\\n\u0026=\u0026 \\prod_{t=1}^{T} P(v(t) | s(t)) \\prod_{t=1}^{T} P(s(t) | s(t-1)) \\end{eqnarray}$$\nWe are getting close to our original equation, just one more step is left. The above equation is for a specific sequence of hidden states that we thought could have generated the visible sequence of symbols. We should compute the probability of all the different possible sequences of hidden states by summing over all the joint probabilities of $V^T$ and $S^T$.\nIn out example, we have a sequence of 3 visible states and 2 hidden states that could emit them. There can be $2^3 = 8$ possible sequences.\nThe generalized equation:\n$$ \\begin{eqnarray} P(V^T|\\theta) \u0026=\u0026 \\sum_{\\text{all seq of S}} P(V^T, S^T) \\\\\\\n\u0026=\u0026 \\sum_{\\text{all seq of S}} P(V^T | S^T)P(S^T) \\\\\\\n\u0026=\u0026 \\sum_{r=1}^R \\prod_{t=1}^{T} P(v(t) | s(t)) \\prod_{t=1}^{T} P(s(t) | s(t-1)) \\\\\\\n\u0026=\u0026 \\sum_{r=1}^R \\prod_{t=1}^{T} P(v(t) | s(t)) P(s(t) | s(t-1)) \\end{eqnarray} $$\nAs previously, $R$ - number of possible sequences of the hidden states.\nA bad news is that the computation complexity of that approach is $O(N^T T)$ which makes it not really practical. We need to find easy to compute alternative and a recursive dynamic programming is the one that can save us from the exponential computation. There are two such algorithms, Forward Algorithm and Backward Algorithm.\nForward algorithm As the name suggests, we want to use the computed probability on current time step to compute the probability of the next time step. The computational complexity is far more efficient than previous $O(N^T T)$.\nTo do this, we need to find the answer the following question - given a sequence of Visible states $V^T$ what is the probability that the Hidden Markov Model will be in a particular hidden state $s$ at a particular time step $t$?\nThe question rewritten as mathematical formula:\n$$ \\alpha_j(t) = p(v(1)…v(t),s(t)= j) = ? $$\nWe’ll start with deriving the equation using probability and then solve it again using trellis diagram. If the former is too dificult, skip it and go straight to the diagram.\nSolution using probabilities Before deriving a general equation, let’s start with calculations for the small lengths of the sequence.\nWhen t = 1\n$$ \\begin{eqnarray} \\alpha_j(1) \u0026=\u0026 p(v_k(1),s(1)= j) \\\\\\\n\u0026=\u0026 p(v_k(1)|s(1)=j)p(s(1)=j) \\\\\\\n\u0026=\u0026 \\pi_j p(v_k(1)|s(1)=j) \\\\\\\n\u0026=\u0026 \\pi_j b_{jk} \\\\\\\n\\text{where } \\pi \u0026=\u0026 \\text{ initial distribution, } \\\\\\\nb_{jkv(1)} \u0026=\u0026 \\text{ emission probability at } t = 1 \\end{eqnarray} $$\nWhen t = 2\nWe have the solution for t=1. Now we want to rewrite the same for t=2 and come up with equation with $\\alpha_j(1)$ as a part of it so we can use recursion.\n$$ \\begin{eqnarray} \\alpha_j(2) \u0026=\u0026 p \\Big( v_k(1),v_k(2),s(2)= j \\Big) \\\\\\\n\u0026=\u0026 {\\sum_{i=1}^M} p \\Big( v_k(1),v_k(2),{s(1)= i}, s(2)= j \\Big) \\\\\\ \u0026=\u0026 \\sum_{i=1}^M p \\Big( v_k(2) | s(2)= j, v_k(1),s(1)= i \\Big) p \\Big( v_k(1),s(2),s(1)= i \\Big)\\\\\\\n\u0026=\u0026 \\sum_{i=1}^M p \\Big( v_k(2) | s(2)= j, {v_k(1), s(1)= i} \\Big) p \\Big( s(2) | {v_k(1),}s(1)= i \\Big) p \\Big(v_k(1),s(1)= i \\Big) \\\\\\\n\u0026=\u0026 \\sum_{i=1}^M p \\Big( v_k(2) | s(2)= j \\Big) p \\Big(s(2) | s(1)= i \\Big) p \\Big(v_k(1),s(1)= i \\Big)\\\\\\\n\u0026=\u0026 {p \\Big( v_k(2) | s(2)= j \\Big) }\\sum_{i=1}^M p \\Big( s(2) | s(1)= i \\Big) {p \\Big( v_k(1),s(1)= i \\Big)} \\\\\\\n\u0026=\u0026 {b_{jk v(2)}} \\sum_{i=1}^M a_{i2} {\\alpha_i(1)}\\\\\\\n\\text{where } a_{i2} \u0026=\u0026 \\text{ transition probability } \\\\\\\nb_{jk v(2)} \u0026=\u0026 \\text{ emission probability at } t=2 \\\\\\\n\\alpha_i(1) \u0026=\u0026 \\text{ forward probability at } t=1 \\end{eqnarray} $$\nGeneralized equation Let’s generalize the equation for any time step t+1:\n$$ \\begin{eqnarray} \\alpha_j(t+1) \u0026=\u0026 p \\Big( v_k(1) … v_k(t+1),s(t+1)= j \\Big) \\\\\\\n\u0026=\u0026 {\\sum_{i=1}^M} p\\Big(v_k(1) … v_k(t+1),{s(t)= i}, s(t+1)= j \\Big) \\\\\\\n\u0026=\u0026 \\sum_{i=1}^M p\\Big(v_k(t+1) | s(t+1)= j, v_k(1) … v_k(t),s(t)= i\\Big) \\cdot \\\\\\\n\u0026\\cdot\u0026 p\\Big(v_k(1)…v_k(t),s(t+1),s(t)= i \\Big) = \\\\\\\n\u0026=\u0026 \\sum_{i=1}^M p\\Big(v_k(t+1) | s(t+1)= j, {v_k(1)…v_k(t), s(t)= i}\\Big) \\cdot \\\\\\\n\u0026\\cdot\u0026 p\\Big(s(t+1) | {v_k(1)…v_k(t),}s(t)= i\\Big) p\\Big(v_k(t),s(t)= i\\Big)\\\\\\\n\u0026=\u0026 \\sum_{i=1}^M p\\Big(v_k(t+1) | s(t+1)= j\\Big) p\\Big(s(t+1) | s(t)= i\\Big) p\\Big(v_k(t),s(t)= i\\Big)\\\\\\ \u0026=\u0026 {p\\Big(v_k(t+1) | s(t+1)= j\\Big) }\\sum_{i=1}^M p\\Big(s(t+1) | s(t)= i\\Big) {p\\Big(v_k(t),s(t)= i\\Big)} \\\\\\\n\u0026=\u0026 {b_{jk v(t+1)}} \\sum_{i=1}^M a_{ij} {\\alpha_i(t)} \\end{eqnarray} $$\nThe equation above follows the same derivation as for t=2. This equation will be really to implement using any programming language. We won’t use recursion function, just the pre-calculated values in a loop.\nIntuition using Trellis We will use Trellis Diagram to get the intuition behind the Forward Algorithm. In case the derivation using joint probability rule was too dificult, this section will definitely help you to understand the equation.\nStating once again the question: given a sequence of visible states $V^T$ what will be the probability that the Hidden Markov Model will be in particular hidden state $s$ at a particular time step $t$?\nStep by step derivation\nLook at the Trellis diagram below and assume the probability that the system/machine is at hidden state $s_1$ at time $(t-1)$ is $\\alpha_1(t-1)$. The probability of transition to hidden state $s_2$ at time step $t$ can be written as $\\alpha_1(t-1)a_{12}$.\nLikewise, if we sum all the probabilities where the machine transitions to state $s_2$ at time t from any state at time $(t-1)$, it gives the total probability that there will be a transition from any hidden state at $(t-1)$ to $s_2$ at time step t.\nMathematically:\n$$ \\sum_{i=1}^M \\alpha_i(t-1) a_{i2} $$\nFinally, we can say the probability that the machine is at hidden state $s_2$ at time t after emitting first t number of visible symbols from sequence $V^T$ is given by the following formula:\n$$ b_{2k} \\sum_{i=1}^M \\alpha_i(t-1) a_{i2} $$\nNow we can extend this to a recursive algorithm to find the probability that sequence $V^T$ was generated by HMM $\\theta$. Here is the generalized version of the equation.\n$$ \\alpha_j(t)= \\begin{cases} \\pi_jb_{jk} \u0026 \\text{ when }t = 1 \\\\\\\nb_{jk} \\sum_{i=1}^M \\alpha_i(t-1) a_{ij} \u0026 \\text{ when } t \\text{ greater than } 1 \\end{cases} $$\nHere $\\alpha_j(t)$ is the probability that the machine will be at hidden state $s_j$ at time step t after emitting first t visible sequence of symbols.\nImplementation of Forward algorithm Now let’s work on the implementation. We’ll use both Python and R for this.\nData\nIn our example we have 2 Hidden States (A, B) and 3 Visible States (0, 1, 2) (in R file it will be (1, 2, 3)). Assume that we already know our a and b.\n$$ A= \\begin{bmatrix} 0.54 \u0026 0.46\\\\\\ 0.49 \u0026 0.51 \\end{bmatrix} $$\n$$ B= \\begin{bmatrix} 0.16 \u0026 0.26 \u0026 0.58\\\\\\\n0.25 \u0026 0.28 \u0026 0.47 \\end{bmatrix} $$\nThe data_python.csv and data_r.csv has two columns named Hidden and Visible. The only difference between the Python and R is only the starting index of the Visible column. Python file has 0, 1, 2 where R has 1, 2, 3.\nPython\nFirst load the data.\nimport pandas as pd import numpy as np data = pd.read_csv('data_python.csv') V = data['Visible'].values Then set the values for transition probability, emission probabilities and initial distribution.\n# Transition Probabilities a = np.array(((0.54, 0.46), (0.49, 0.51))) # Emission Probabilities b = np.array(((0.16, 0.26, 0.58), (0.25, 0.28, 0.47))) # Equal Probabilities for the initial distribution initial_distribution = np.array((0.5, 0.5)) In python the index starts from 0, hence our t will start from 0 to T-1.\nNext, we will have the forward function. Here we will store and return all the $\\alpha_0(0), \\ \\alpha_1(0) \\ … \\ \\alpha_0(T-1), \\ \\alpha_1(T-1)$\ndef forward(V, a, b, initial_distribution): alpha = np.zeros((V.shape[0], a.shape[0])) alpha[0, :] = initial_distribution * b[:, V[0]] for t in range(1, V.shape[0]): for j in range(a.shape[0]): # Matrix Computation Steps # ((1x2) . (1x2)) * (1) # (1) * (1) alpha[t, j] = alpha[t - 1].dot(a[:, j]) * b[j, V[t]] return alpha alpha = forward(V, a, b, initial_distribution) print(alpha) First we will create the alpha matrix with 2 columns and T Rows.\nAs per our equation multiply initial_distribution with the $b_{jkv(0)}$ to calculate $\\alpha_0(0) , \\ \\alpha_1(0)$. This will be a simple vector multiplication since both initial_distribution and $b_{kv(0)}$ are of same size.\n We will loop through the time steps now, starting from 1 (remember python index starts from 0). Another loop for each hidden step j. Use the same formula for calculating the $\\alpha$ values. Return all of the alpha values.  Output\n[[8.00000000e-002 1.25000000e-001] [[8.00000000e-002 1.25000000e-001] [2.71570000e-002 2.81540000e-002] [1.65069392e-002 1.26198572e-002] [8.75653677e-003 6.59378003e-003] … … [8.25847348e-221 6.30684489e-221] [4.37895921e-221 3.29723269e-221] [1.03487332e-221 1.03485477e-221] [6.18228050e-222 4.71794300e-222]] R\nHere is the same Forward Algorithm implemented in R. If you notice, we have removed the 2nd for loop in R code. You can do the same in python.\ndata = read.csv(\"data_r.csv\") a = matrix(c(0.54, 0.49, 0.46, 0.51),nrow = 2,ncol = 2) b = matrix(c(0.16, 0.25, 0.26, 0.28, 0.58, 0.47),nrow = 2,ncol = 3) initial_distribution = c(1/2, 1/2) forward = function(v, a, b, initial_distribution){ T = length(v) m = nrow(a) alpha = matrix(0, T, m) alpha[1, ] = initial_distribution*b[, v[1]] for(t in 2:T){ tmp = alpha[t-1, ] %*% a alpha[t, ] = tmp * b[, v[t]] } return(alpha) } forward(data$Visible,a,b,initial_distribution) Backward algorithm Backward Algorithm is the time-reversed version of the Forward Algorithm. In Backward Algorithm we need to find the probability that the machine will be in hidden state $s_i$ at time step t and will generate the remaining part of the sequence of the visible symbol $V^T$.\nDerivation of Backward algorithm The concepts are same as when we were deriving the forward algorithm using Probability Theory.\n$$ \\begin{eqnarray} \\beta_i(t) \u0026=\u0026 p \\Big( v_k(t+1) …. v_k(T) | s(t) = i \\Big) \\\\\\ \u0026=\u0026 \\sum_{j=0}^M p\\Big( v_k(t+1) …. v_k(T), s(t+1) = j | s(t) = i \\Big) \\\\\\ \u0026=\u0026 \\sum_{j=0}^M p\\Big( v_k(t+2) …. v_k(T) | v_k(t+1) , s(t+1) = j , s(t) = i \\Big) \\cdot \\\\\\ \u0026\\cdot\u0026 p \\Big( v_k(t+1) , s(t+1) = j | s(t) = i \\Big) \\\\\\ \u0026=\u0026 \\sum_{j=0}^M p\\Big( v_k(t+2) …. v_k(T) | v_k(t+1) , s(t+1) = j , s(t) = i \\Big) \\cdot \\\\\\ \u0026\\cdot\u0026\tp \\Big( v_k(t+1) | s(t+1) = j , s(t) = i \\Big) p \\Big( s(t+1) = j | s(t) = i \\Big) \\\\\\ \u0026=\u0026 \\sum_{j=0}^M p\\Big( v_k(t+2) …. v_k(T) | s(t+1) = j \\Big) p \\Big( v_k(t+1) | s(t+1) = j \\Big) \\cdot \\\\\\\n\u0026\\cdot\u0026 p \\Big( s(t+1) = j | s(t) = i \\Big) \\\\\\ \u0026=\u0026 \\sum_{j=0}^M \\beta_j(t+1) b_{jkv(t+1)} a_{ij} \\\\\\ \\text{where } a_{i2} \u0026=\u0026 \\text{ Transition Probability } \\\\\\ b_{jk v(t+1)} \u0026=\u0026 \\text{ Emission Probability at } t=t+1 \\\\\\ \\beta_i(t+1) \u0026=\u0026 \\text{ Backward probability at } t=t+1 \\end{eqnarray} $$\nIntuition using Trellis Here is the Trellis diagram of the Backward Algorithm. Mathematically, the algorithm can be written in following way:\n$$ \\beta_i(t)= \\begin{cases} 1 \u0026 \\text{ when }t = T \\\\\\ \\sum_{j=0}^M a_{ij} b_{jkv(t+1)}\\beta_j(t+1) \u0026 \\text{ when } t \\text{ less than } T \\end{cases} $$\nImplementation of Backward algorithm We will use the same data file and parameters as defined for Forward Algorithm.\nPython code\nimport pandas as pd import numpy as np data = pd.read_csv('data_python.csv') V = data['Visible'].values # Transition Probabilities a = np.array(((0.54, 0.46), (0.49, 0.51))) # Emission Probabilities b = np.array(((0.16, 0.26, 0.58), (0.25, 0.28, 0.47))) def backward(V, a, b): beta = np.zeros((V.shape[0], a.shape[0])) # setting beta(T) = 1 beta[V.shape[0] - 1] = np.ones((a.shape[0])) # Loop in backward way from T-1 to # Due to python indexing the actual loop will be T-2 to 0 for t in range(V.shape[0] - 2, -1, -1): for j in range(a.shape[0]): beta[t, j] = (beta[t + 1] * b[:, V[t + 1]]).dot(a[j, :]) return beta beta = backward(V, a, b) print(beta) R code\ndata = read.csv(\"data_r.csv\") a = matrix(c(0.54, 0.49, 0.46, 0.51),nrow = 2,ncol = 2) b = matrix(c(0.16, 0.25, 0.26, 0.28, 0.58, 0.47),nrow = 2,ncol = 3) backward = function(V, A, B){ T = length(V) m = nrow(A) beta = matrix(1, T, m) for(t in (T-1):1){ tmp = as.matrix(beta[t+1, ] * B[, V[t+1]]) beta[t, ] = t(A %*% tmp) } return(beta) } backward(data$Visible,a,b) Output\n[[5.30694627e-221 5.32373319e-221] [1.98173335e-220 1.96008747e-220] [3.76013005e-220 3.71905927e-220] [7.13445025e-220 7.05652279e-220] ... ... [7.51699476e-002 7.44006456e-002] [1.41806080e-001 1.42258480e-001] [5.29400000e-001 5.23900000e-001] [1.00000000e+000 1.00000000e+000]] Conclusion In the next article we will use both forward and backward algorithm to solve the learning problem. Here we have provided a very detailed overview of the Forward and Backward Algorithm. The output of the program may not make a lot of sense now however the next article will provide more insight.\n","wordCount":"2618","inLanguage":"en","datePublished":"2021-10-26T00:09:35+02:00","dateModified":"2021-10-26T00:09:35+02:00","author":{"@type":"Person","name":"Marcin Halupka"},"mainEntityOfPage":{"@type":"WebPage","@id":"/posts/forward-and-backward-algorithm-in-hidden-markov-model/"},"publisher":{"@type":"Organization","name":"Marcin Halupka","logo":{"@type":"ImageObject","url":"%3Clink%20/%20abs%20url%3E"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href accesskey=h title="Marcin Halupka (Alt + H)">Marcin Halupka</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=/posts/ title=Posts>
<span>Posts</span>
</a>
</li>
<li>
<a href=/projects/ title=Projects>
<span>Projects</span>
</a>
</li>
<li>
<a href=/about/ title="About me">
<span>About me</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href>Home</a>&nbsp;»&nbsp;<a href=/posts/>Posts</a></div>
<h1 class=post-title>
Forward and Backward Algorithm in Hidden Markov Model
</h1>
<div class=post-meta>October 26, 2021&nbsp;·&nbsp;13 min&nbsp;·&nbsp;Marcin Halupka&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/forward-and-backward-algorithm-in-hidden-markov-model.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header> <div class=toc>
<details open>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#quick-recap aria-label="Quick recap">Quick recap</a><ul>
<li>
<a href=#basic-structure-of-hmm aria-label="Basic structure of HMM">Basic structure of HMM</a></li></ul>
</li>
<li>
<a href=#evaluation-problem aria-label="Evaluation problem">Evaluation problem</a><ul>
<li>
<a href=#solution aria-label=Solution>Solution</a></li>
<li>
<a href=#how-to-proof-the-solution-is-valid aria-label="How to proof the solution is valid?">How to proof the solution is valid?</a></li></ul>
</li>
<li>
<a href=#forward-algorithm aria-label="Forward algorithm">Forward algorithm</a><ul>
<li>
<a href=#solution-using-probabilities aria-label="Solution using probabilities">Solution using probabilities</a></li>
<li>
<a href=#generalized-equation aria-label="Generalized equation">Generalized equation</a></li>
<li>
<a href=#intuition-using-trellis aria-label="Intuition using Trellis">Intuition using Trellis</a></li>
<li>
<a href=#implementation-of-forward-algorithm aria-label="Implementation of Forward algorithm">Implementation of Forward algorithm</a></li></ul>
</li>
<li>
<a href=#backward-algorithm aria-label="Backward algorithm">Backward algorithm</a><ul>
<li>
<a href=#derivation-of-backward-algorithm aria-label="Derivation of Backward algorithm">Derivation of Backward algorithm</a></li>
<li>
<a href=#intuition-using-trellis-1 aria-label="Intuition using Trellis">Intuition using Trellis</a></li>
<li>
<a href=#implementation-of-backward-algorithm aria-label="Implementation of Backward algorithm">Implementation of Backward algorithm</a></li></ul>
</li>
<li>
<a href=#conclusion aria-label=Conclusion>Conclusion</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p>Introduction to Hidden Markov Model provided basic understanding of the topic. We also presented three main problems of HMM (<code>Evaluation</code>, <code>Learning</code> and <code>Decoding</code>).
In this post we&rsquo;ll deep dive into the <strong>Evaluation Problem</strong>. Starting from mathematical understanding, finishing on Python and R implementations.</p>
<h1 id=quick-recap>Quick recap<a hidden class=anchor aria-hidden=true href=#quick-recap>#</a></h1>
<p>Hidden Markov Model is a <strong>Markov Chain</strong> which is mainly used in problems with <strong>temporal sequence</strong> of the data. Markov Model explaimns that the next step depends only on the previous step in
a temporal sequence. In Hidden Markov Model the state of the system is <code>hidden</code> however each state emits a visible symbol at every time step. HMM can work with both discrete and continuous sequence of the data
but we&rsquo;ll focus on the former.</p>
<h2 id=basic-structure-of-hmm>Basic structure of HMM<a hidden class=anchor aria-hidden=true href=#basic-structure-of-hmm>#</a></h2>
<p>Hidden Markov Model ($\theta$) has the following parameters:</p>
<ul>
<li>Set of $M$ Hidden States ($S^M$)</li>
<li>A Transaction Probability Matrix ($A$)</li>
<li>A sequence of T observations ($V^T$)</li>
<li>An Emission Probability Matrix ($B$)</li>
<li>An initial probability distribution ($\pi$)</li>
</ul>
<h1 id=evaluation-problem>Evaluation problem<a hidden class=anchor aria-hidden=true href=#evaluation-problem>#</a></h1>
<p>As we have seen before, the Evaluation Problem can be stated as follows:</p>
<p>$$
\displaylines{
\text{Given } \theta, V_T \rightarrow \text{Estimate } P(V_T|\theta) \\\<br>
\text{Where } \theta \rightarrow s, v, a_{ij},b_{jk}
}
$$</p>
<h2 id=solution>Solution<a hidden class=anchor aria-hidden=true href=#solution>#</a></h2>
<ol>
<li>First we need to find all possible sequences of the states $S^M$ where <code>M</code> is the number of Hidden States</li>
<li>Then from all those sequences of $S^M$, find the probability of which sequence generated the visible sequence of symbols $V^T$</li>
<li>Mathematically $P(V_T|\theta)$ can be estimated as $p(V^T|\theta)= \sum_{r=1}^{R} p(V^T|S_r^T)p(S_r^T), \, \, \text{where }S_r^T = { s_1(1), s_2(2)… s_r(T)}$<br>
and $R$ - maximum number of possivle sequences of the hidden state.</li>
</ol>
<p>If there are $M$ possible hidden states, then $R = M^T$.</p>
<p>In order to compute the probability of the model generated by the <em>particular</em> sequence of <code>T</code> visible symbols $V^T$, we should take each conceivable sequence of hidden states, calculate probability that they
have produced $V^T$ and then add up these probabilities.</p>
<h2 id=how-to-proof-the-solution-is-valid>How to proof the solution is valid?<a hidden class=anchor aria-hidden=true href=#how-to-proof-the-solution-is-valid>#</a></h2>
<p>Let&rsquo;s present it in a different way.</p>
<p>Coming back to one of our previous examples, the diagram below presents a sequence of 3 states. The transition probabilities between hidden states are unknown.</p>
<p><img loading=lazy src=/posts/images/3/transitions-visible.png alt="Transitions between hidden layers with unknown probabilities">
</p>
<p>In case in the example above we already know the sequence of the hidden states (<code>sun</code>, <code>sun</code>, <code>cloud</code>) which generated the 3 visible symbols (<code>happy</code>, <code>sad</code>, <code>happy</code>).</p>
<p>Calculation of the probability of the visible symbols given the hidden states is pretty straightforward.</p>
<p>$$
P(\text{happy, sad, happy} | \text{sun, sun, rain}) = P(\text{happy} | \text{sun}) \times P(\text{sad} | \text{sun}) \times P(\text{happy} | \text{rain})
$$</p>
<p>Mathematically probability of $V^T$ given $S^T$ can be written as:</p>
<p>$$
p(V^T|S_r^T)=\prod_{t=1}^{T} p(v(t) | s(t))
$$</p>
<p>Unfortunately we <strong>don&rsquo;t know the specific sequence of hidden states</strong> that generated the visible symbols. We need to compute the probability of mood changes
(<code>happy</code>, <code>sad</code>, <code>happy</code>) by <strong>summing over all possible weather sequences</strong> weighted by their probability (transition probability).</p>
<p><img loading=lazy src=/posts/images/3/transitions-hidden.png alt="Transitions between hidden layers known with probabilities">
</p>
<p>We can calculate the <strong>joint probability</strong> of the sequence of visible symbol $V^T$ generated by a specific sequence of hidden state $S^T$ as:</p>
<p>$$
\displaylines{
P(\text{happy, sad, happy} | \text{sun, sun, rain}) = P(\text{sun} | \text{initial state}) \times P(\text{sun} | \text{sun}) \times \\\<br>
\times P(\text{rain} | \text{sun}) \times P(\text{happy} | \text{sun}) \times P(\text{sad} | \text{sun}) \times P(\text{happy} | \text{rain})
}
$$</p>
<p>In general:</p>
<p>$$
P(V^T,S^T)=P(V^T | S^T)P(S^T)
$$</p>
<p>Since we&rsquo;re using <strong>First-Order Markov Model</strong>, we can say that the probability of a sequence of <code>T</code> hidden states is the multiplication of the probability of each transition:</p>
<p>$$
P(S^T)=\prod_{t=1}^{T} P(s(t) | s(t-1))
$$</p>
<p>So the joint probability can be written as:</p>
<p>$$\begin{eqnarray}
P(V^T,S^T) &=& P(V^T | S^T)P(S^T) \\\<br>
&=& \prod_{t=1}^{T} P(v(t) | s(t)) \prod_{t=1}^{T} P(s(t) | s(t-1))
\end{eqnarray}$$</p>
<p>We are getting close to our original equation, just one more step is left. The above equation is for a <strong>specific sequence</strong> of hidden states that we thought could have generated
the visible sequence of symbols. We should compute the probability of <strong>all</strong> the different possible sequences of hidden states by <strong>summing over all the joint probabilities</strong> of
$V^T$ and $S^T$.</p>
<p>In out example, we have a sequence of 3 visible states and 2 hidden states that could emit them. There can be $2^3 = 8$ possible sequences.</p>
<p>The generalized equation:</p>
<p>$$
\begin{eqnarray}
P(V^T|\theta) &=& \sum_{\text{all seq of S}} P(V^T, S^T) \\\<br>
&=& \sum_{\text{all seq of S}} P(V^T | S^T)P(S^T) \\\<br>
&=& \sum_{r=1}^R \prod_{t=1}^{T} P(v(t) | s(t)) \prod_{t=1}^{T} P(s(t) | s(t-1)) \\\<br>
&=& \sum_{r=1}^R \prod_{t=1}^{T} P(v(t) | s(t)) P(s(t) | s(t-1))
\end{eqnarray}
$$</p>
<p>As previously, $R$ - number of possible sequences of the hidden states.</p>
<p>A bad news is that the <strong>computation complexity</strong> of that approach is $O(N^T T)$ which makes it not really practical. We need to find easy to compute alternative and a <strong>recursive dynamic programming</strong>
is the one that can save us from the exponential computation. There are two such algorithms, <code>Forward Algorithm</code> and <code>Backward Algorithm</code>.</p>
<h1 id=forward-algorithm>Forward algorithm<a hidden class=anchor aria-hidden=true href=#forward-algorithm>#</a></h1>
<p>As the name suggests, we want to use the computed probability on <strong>current time step</strong> to compute the probability of the <strong>next time step</strong>. The computational complexity is far more efficient than previous $O(N^T T)$.</p>
<p>To do this, we need to find the answer the following question - given a sequence of Visible states $V^T$ what is the probability that the Hidden Markov Model will be in a particular hidden state $s$ at a particular
time step $t$?</p>
<p>The question rewritten as mathematical formula:</p>
<p>$$
\alpha_j(t) = p(v(1)…v(t),s(t)= j) = ?
$$</p>
<p>We&rsquo;ll start with deriving the equation using probability and then solve it again using <code>trellis diagram</code>. If the former is too dificult, skip it and go straight to the diagram.</p>
<h2 id=solution-using-probabilities>Solution using probabilities<a hidden class=anchor aria-hidden=true href=#solution-using-probabilities>#</a></h2>
<p>Before deriving a general equation, let&rsquo;s start with calculations for the small lengths of the sequence.</p>
<p><strong>When <code>t = 1</code></strong></p>
<p>$$
\begin{eqnarray}
\alpha_j(1) &=& p(v_k(1),s(1)= j) \\\<br>
&=& p(v_k(1)|s(1)=j)p(s(1)=j) \\\<br>
&=& \pi_j p(v_k(1)|s(1)=j) \\\<br>
&=& \pi_j b_{jk} \\\<br>
\text{where } \pi &=& \text{ initial distribution, } \\\<br>
b_{jkv(1)} &=& \text{ emission probability at } t = 1
\end{eqnarray}
$$</p>
<p><strong>When <code>t = 2</code></strong></p>
<p>We have the solution for <code>t=1</code>. Now we want to rewrite the same for <code>t=2</code> and come up with equation with $\alpha_j(1)$ as a part of it so we can use <code>recursion</code>.</p>
<p>$$
\begin{eqnarray}
\alpha_j(2) &=& p \Big( v_k(1),v_k(2),s(2)= j \Big) \\\<br>
&=& {\sum_{i=1}^M} p \Big( v_k(1),v_k(2),{s(1)= i}, s(2)= j \Big) \\\
&=& \sum_{i=1}^M p \Big( v_k(2) | s(2)= j, v_k(1),s(1)= i \Big) p \Big( v_k(1),s(2),s(1)= i \Big)\\\<br>
&=& \sum_{i=1}^M p \Big( v_k(2) | s(2)= j, {v_k(1), s(1)= i} \Big) p \Big( s(2) | {v_k(1),}s(1)= i \Big) p \Big(v_k(1),s(1)= i \Big) \\\<br>
&=& \sum_{i=1}^M p \Big( v_k(2) | s(2)= j \Big) p \Big(s(2) | s(1)= i \Big) p \Big(v_k(1),s(1)= i \Big)\\\<br>
&=& {p \Big( v_k(2) | s(2)= j \Big) }\sum_{i=1}^M p \Big( s(2) | s(1)= i \Big) {p \Big( v_k(1),s(1)= i \Big)} \\\<br>
&=& {b_{jk v(2)}} \sum_{i=1}^M a_{i2} {\alpha_i(1)}\\\<br>
\text{where } a_{i2} &=& \text{ transition probability } \\\<br>
b_{jk v(2)} &=& \text{ emission probability at } t=2 \\\<br>
\alpha_i(1) &=& \text{ forward probability at } t=1
\end{eqnarray}
$$</p>
<h2 id=generalized-equation>Generalized equation<a hidden class=anchor aria-hidden=true href=#generalized-equation>#</a></h2>
<p>Let&rsquo;s generalize the equation for any time step <code>t+1</code>:</p>
<p>$$
\begin{eqnarray}
\alpha_j(t+1) &=& p \Big( v_k(1) … v_k(t+1),s(t+1)= j \Big) \\\<br>
&=& {\sum_{i=1}^M} p\Big(v_k(1) … v_k(t+1),{s(t)= i}, s(t+1)= j \Big) \\\<br>
&=& \sum_{i=1}^M p\Big(v_k(t+1) | s(t+1)= j, v_k(1) … v_k(t),s(t)= i\Big) \cdot \\\<br>
&\cdot& p\Big(v_k(1)…v_k(t),s(t+1),s(t)= i \Big) = \\\<br>
&=& \sum_{i=1}^M p\Big(v_k(t+1) | s(t+1)= j, {v_k(1)…v_k(t), s(t)= i}\Big) \cdot \\\<br>
&\cdot& p\Big(s(t+1) | {v_k(1)…v_k(t),}s(t)= i\Big) p\Big(v_k(t),s(t)= i\Big)\\\<br>
&=& \sum_{i=1}^M p\Big(v_k(t+1) | s(t+1)= j\Big) p\Big(s(t+1) | s(t)= i\Big) p\Big(v_k(t),s(t)= i\Big)\\\
&=& {p\Big(v_k(t+1) | s(t+1)= j\Big) }\sum_{i=1}^M p\Big(s(t+1) | s(t)= i\Big) {p\Big(v_k(t),s(t)= i\Big)} \\\<br>
&=& {b_{jk v(t+1)}} \sum_{i=1}^M a_{ij} {\alpha_i(t)}
\end{eqnarray}
$$</p>
<p>The equation above follows the same derivation as for <code>t=2</code>. This equation will be really to implement using any programming language. We won&rsquo;t use
recursion function, just the pre-calculated values in a loop.</p>
<h2 id=intuition-using-trellis>Intuition using Trellis<a hidden class=anchor aria-hidden=true href=#intuition-using-trellis>#</a></h2>
<p>We will use <code>Trellis Diagram</code> to get the intuition behind the Forward Algorithm. In case the derivation using joint probability rule was too dificult,
this section will definitely help you to understand the equation.</p>
<p>Stating once again the question: given a sequence of visible states $V^T$ what will be the probability that the Hidden Markov Model will be in
particular hidden state $s$ at a particular time step $t$?</p>
<p><strong>Step by step derivation</strong></p>
<p>Look at the Trellis diagram below and assume the probability that the system/machine is at hidden state $s_1$ at time $(t-1)$ is $\alpha_1(t-1)$.
The probability of transition to hidden state $s_2$ at time step $t$ can be written as $\alpha_1(t-1)a_{12}$.</p>
<p><img loading=lazy src=/posts/images/7_trellis_forward.png alt="Graph 3">
</p>
<p>Likewise, if we <code>sum</code> all the probabilities where the machine transitions to state $s_2$ at time <code>t</code> from any state at time $(t-1)$, it gives the total
probability that <code>there will be a transition from any hidden state</code> at $(t-1)$ to $s_2$ at time step <code>t</code>.</p>
<p>Mathematically:</p>
<p>$$
\sum_{i=1}^M \alpha_i(t-1) a_{i2}
$$</p>
<p>Finally, we can say the probability that the machine is at hidden state $s_2$ at time <code>t</code> after emitting first <code>t</code> number of visible symbols from
sequence $V^T$ is given by the following formula:</p>
<p>$$
b_{2k} \sum_{i=1}^M \alpha_i(t-1) a_{i2}
$$</p>
<p>Now we can extend this to a <code>recursive algorithm</code> to find the probability that sequence $V^T$ was generated by HMM $\theta$. Here is the generalized
version of the equation.</p>
<p>$$
\alpha_j(t)= \begin{cases}
\pi_jb_{jk} & \text{ when }t = 1 \\\<br>
b_{jk} \sum_{i=1}^M \alpha_i(t-1) a_{ij} & \text{ when } t \text{ greater than } 1
\end{cases}
$$</p>
<p>Here $\alpha_j(t)$ is the probability that the machine will be at hidden state $s_j$ at time step <code>t</code> after emitting first <code>t</code> visible sequence of symbols.</p>
<h2 id=implementation-of-forward-algorithm>Implementation of Forward algorithm<a hidden class=anchor aria-hidden=true href=#implementation-of-forward-algorithm>#</a></h2>
<p>Now let&rsquo;s work on the implementation. We&rsquo;ll use both Python and R for this.</p>
<p><strong>Data</strong></p>
<p>In our example we have 2 Hidden States <code>(A, B)</code> and 3 Visible States <code>(0, 1, 2)</code> (in R file it will be <code>(1, 2, 3)</code>). Assume that we already know our <code>a</code> and <code>b</code>.</p>
<p>$$
A=
\begin{bmatrix}
0.54 & 0.46\\\
0.49 & 0.51
\end{bmatrix}
$$</p>
<p>$$
B= \begin{bmatrix}
0.16 & 0.26 & 0.58\\\<br>
0.25 & 0.28 & 0.47
\end{bmatrix}
$$</p>
<p>The <code>data_python.csv</code> and <code>data_r.csv</code> has two columns named <code>Hidden</code> and <code>Visible</code>. The only difference between the Python and R is only the starting index of the Visible column.
Python file has 0, 1, 2 where R has 1, 2, 3.</p>
<p><strong>Python</strong></p>
<p>First load the data.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
<span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
 
data <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#39;data_python.csv&#39;</span>)
 
V <span style=color:#f92672>=</span> data[<span style=color:#e6db74>&#39;Visible&#39;</span>]<span style=color:#f92672>.</span>values
</code></pre></div><p>Then set the values for transition probability, emission probabilities and initial distribution.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Transition Probabilities</span>
a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(((<span style=color:#ae81ff>0.54</span>, <span style=color:#ae81ff>0.46</span>), (<span style=color:#ae81ff>0.49</span>, <span style=color:#ae81ff>0.51</span>)))
 
<span style=color:#75715e># Emission Probabilities</span>
b <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(((<span style=color:#ae81ff>0.16</span>, <span style=color:#ae81ff>0.26</span>, <span style=color:#ae81ff>0.58</span>), (<span style=color:#ae81ff>0.25</span>, <span style=color:#ae81ff>0.28</span>, <span style=color:#ae81ff>0.47</span>)))
 
<span style=color:#75715e># Equal Probabilities for the initial distribution</span>
initial_distribution <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array((<span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.5</span>))
</code></pre></div><p>In python the index starts from 0, hence our <code>t</code> will start from <code>0</code> to <code>T-1</code>.</p>
<p>Next, we will have the <code>forward function</code>. Here we will store and return all the $\alpha_0(0), \ \alpha_1(0) \ … \ \alpha_0(T-1), \ \alpha_1(T-1)$</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(V, a, b, initial_distribution):
    alpha <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((V<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], a<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]))
    alpha[<span style=color:#ae81ff>0</span>, :] <span style=color:#f92672>=</span> initial_distribution <span style=color:#f92672>*</span> b[:, V[<span style=color:#ae81ff>0</span>]]
 
    <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, V<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(a<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
            <span style=color:#75715e># Matrix Computation Steps</span>
            <span style=color:#75715e>#                  ((1x2) . (1x2))      *     (1)</span>
            <span style=color:#75715e>#                        (1)            *     (1)</span>
            alpha[t, j] <span style=color:#f92672>=</span> alpha[t <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>dot(a[:, j]) <span style=color:#f92672>*</span> b[j, V[t]]
 
    <span style=color:#66d9ef>return</span> alpha
 
alpha <span style=color:#f92672>=</span> forward(V, a, b, initial_distribution)
print(alpha)
</code></pre></div><p>First we will create the alpha matrix with <code>2 columns</code> and <code>T Rows</code>.<br>
As per our equation multiply <code>initial_distribution</code> with the $b_{jkv(0)}$ to calculate $\alpha_0(0) , \ \alpha_1(0)$. This will be a simple vector multiplication since both <code>initial_distribution</code> and $b_{kv(0)}$
are of same size.</p>
<ul>
<li>We will loop through the time steps now, starting from 1 (remember python index starts from 0).</li>
<li>Another loop for each hidden step <code>j</code>.</li>
<li>Use the same formula for calculating the $\alpha$ values.</li>
<li>Return all of the alpha values.</li>
</ul>
<p><strong>Output</strong></p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>[[<span style=color:#ae81ff>8.00000000e-002</span> <span style=color:#ae81ff>1.25000000e-001</span>]
[[<span style=color:#ae81ff>8.00000000e-002</span> <span style=color:#ae81ff>1.25000000e-001</span>]
 [<span style=color:#ae81ff>2.71570000e-002</span> <span style=color:#ae81ff>2.81540000e-002</span>]
 [<span style=color:#ae81ff>1.65069392e-002</span> <span style=color:#ae81ff>1.26198572e-002</span>]
 [<span style=color:#ae81ff>8.75653677e-003</span> <span style=color:#ae81ff>6.59378003e-003</span>]
<span style=color:#960050;background-color:#1e0010>…</span>
<span style=color:#960050;background-color:#1e0010>…</span>
 [<span style=color:#ae81ff>8.25847348e-221</span> <span style=color:#ae81ff>6.30684489e-221</span>]
 [<span style=color:#ae81ff>4.37895921e-221</span> <span style=color:#ae81ff>3.29723269e-221</span>]
 [<span style=color:#ae81ff>1.03487332e-221</span> <span style=color:#ae81ff>1.03485477e-221</span>]
 [<span style=color:#ae81ff>6.18228050e-222</span> <span style=color:#ae81ff>4.71794300e-222</span>]]
</code></pre></div><p><strong>R</strong></p>
<p>Here is the same Forward Algorithm implemented in R. If you notice, we have removed the 2nd for loop in R code. You can do the same in python.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r>data <span style=color:#f92672>=</span> <span style=color:#a6e22e>read.csv</span>(<span style=color:#e6db74>&#34;data_r.csv&#34;</span>)
 
a <span style=color:#f92672>=</span> <span style=color:#a6e22e>matrix</span>(<span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>0.54</span>, <span style=color:#ae81ff>0.49</span>, <span style=color:#ae81ff>0.46</span>, <span style=color:#ae81ff>0.51</span>),nrow <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>,ncol <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>)
b <span style=color:#f92672>=</span> <span style=color:#a6e22e>matrix</span>(<span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>0.16</span>, <span style=color:#ae81ff>0.25</span>, <span style=color:#ae81ff>0.26</span>, <span style=color:#ae81ff>0.28</span>, <span style=color:#ae81ff>0.58</span>, <span style=color:#ae81ff>0.47</span>),nrow <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>,ncol <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>)
initial_distribution <span style=color:#f92672>=</span> <span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>/</span><span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span><span style=color:#f92672>/</span><span style=color:#ae81ff>2</span>)
 
forward <span style=color:#f92672>=</span> <span style=color:#a6e22e>function</span>(v, a, b, initial_distribution){
  
  T <span style=color:#f92672>=</span> <span style=color:#a6e22e>length</span>(v)
  m <span style=color:#f92672>=</span> <span style=color:#a6e22e>nrow</span>(a)
  alpha <span style=color:#f92672>=</span> <span style=color:#a6e22e>matrix</span>(<span style=color:#ae81ff>0</span>, T, m)
  
  alpha[1, ] <span style=color:#f92672>=</span> initial_distribution<span style=color:#f92672>*</span>b[, v[1]]
  
  <span style=color:#a6e22e>for</span>(t in <span style=color:#ae81ff>2</span><span style=color:#f92672>:</span>T){
    tmp <span style=color:#f92672>=</span> alpha[t<span style=color:#ae81ff>-1</span>, ] <span style=color:#f92672>%*%</span> a
    alpha[t, ] <span style=color:#f92672>=</span> tmp <span style=color:#f92672>*</span> b[, v[t]]
  }
  <span style=color:#a6e22e>return</span>(alpha)
}
 
<span style=color:#a6e22e>forward</span>(data<span style=color:#f92672>$</span>Visible,a,b,initial_distribution)
</code></pre></div><h1 id=backward-algorithm>Backward algorithm<a hidden class=anchor aria-hidden=true href=#backward-algorithm>#</a></h1>
<p>Backward Algorithm is the <code>time-reversed</code> version of the Forward Algorithm. In Backward Algorithm we need to find the probability that the machine will be in hidden state $s_i$ at time step <code>t</code>
and will generate the <code>remaining part of the sequence</code> of the visible symbol $V^T$.</p>
<h2 id=derivation-of-backward-algorithm>Derivation of Backward algorithm<a hidden class=anchor aria-hidden=true href=#derivation-of-backward-algorithm>#</a></h2>
<p>The concepts are same as when we were deriving the forward algorithm using Probability Theory.</p>
<p>$$
\begin{eqnarray}
\beta_i(t) &=& p \Big( v_k(t+1) …. v_k(T) | s(t) = i \Big) \\\
&=& \sum_{j=0}^M p\Big( v_k(t+1) …. v_k(T), s(t+1) = j | s(t) = i \Big) \\\
&=& \sum_{j=0}^M p\Big( v_k(t+2) …. v_k(T) | v_k(t+1) , s(t+1) = j , s(t) = i \Big) \cdot \\\
&\cdot& p \Big( v_k(t+1) , s(t+1) = j | s(t) = i \Big) \\\
&=& \sum_{j=0}^M p\Big( v_k(t+2) …. v_k(T) | v_k(t+1) , s(t+1) = j , s(t) = i \Big) \cdot \\\
&\cdot& p \Big( v_k(t+1) | s(t+1) = j , s(t) = i \Big) p \Big( s(t+1) = j | s(t) = i \Big) \\\
&=& \sum_{j=0}^M p\Big( v_k(t+2) …. v_k(T) | s(t+1) = j \Big) p \Big( v_k(t+1) | s(t+1) = j \Big) \cdot \\\<br>
&\cdot& p \Big( s(t+1) = j | s(t) = i \Big) \\\
&=& \sum_{j=0}^M \beta_j(t+1) b_{jkv(t+1)} a_{ij} \\\
\text{where } a_{i2} &=& \text{ Transition Probability } \\\
b_{jk v(t+1)} &=& \text{ Emission Probability at } t=t+1 \\\
\beta_i(t+1) &=& \text{ Backward probability at } t=t+1
\end{eqnarray}
$$</p>
<h2 id=intuition-using-trellis-1>Intuition using Trellis<a hidden class=anchor aria-hidden=true href=#intuition-using-trellis-1>#</a></h2>
<p>Here is the Trellis diagram of the Backward Algorithm. Mathematically, the algorithm can be written in following way:</p>
<p>$$
\beta_i(t)= \begin{cases}
1 & \text{ when }t = T \\\
\sum_{j=0}^M a_{ij} b_{jkv(t+1)}\beta_j(t+1) & \text{ when } t \text{ less than } T
\end{cases}
$$</p>
<p><img loading=lazy src=/posts/images/8_trellis_backward.png alt="Graph 4">
</p>
<h2 id=implementation-of-backward-algorithm>Implementation of Backward algorithm<a hidden class=anchor aria-hidden=true href=#implementation-of-backward-algorithm>#</a></h2>
<p>We will use the same data file and parameters as defined for Forward Algorithm.</p>
<p><strong>Python code</strong></p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
<span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
 
data <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#39;data_python.csv&#39;</span>)
 
V <span style=color:#f92672>=</span> data[<span style=color:#e6db74>&#39;Visible&#39;</span>]<span style=color:#f92672>.</span>values
 
<span style=color:#75715e># Transition Probabilities</span>
a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(((<span style=color:#ae81ff>0.54</span>, <span style=color:#ae81ff>0.46</span>), (<span style=color:#ae81ff>0.49</span>, <span style=color:#ae81ff>0.51</span>)))
 
<span style=color:#75715e># Emission Probabilities</span>
b <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(((<span style=color:#ae81ff>0.16</span>, <span style=color:#ae81ff>0.26</span>, <span style=color:#ae81ff>0.58</span>), (<span style=color:#ae81ff>0.25</span>, <span style=color:#ae81ff>0.28</span>, <span style=color:#ae81ff>0.47</span>)))
 
 
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>backward</span>(V, a, b):
    beta <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((V<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], a<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]))
 
    <span style=color:#75715e># setting beta(T) = 1</span>
    beta[V<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>ones((a<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]))
 
    <span style=color:#75715e># Loop in backward way from T-1 to</span>
    <span style=color:#75715e># Due to python indexing the actual loop will be T-2 to 0</span>
    <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> range(V<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>-</span> <span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>):
        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(a<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
            beta[t, j] <span style=color:#f92672>=</span> (beta[t <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>] <span style=color:#f92672>*</span> b[:, V[t <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>]])<span style=color:#f92672>.</span>dot(a[j, :])
 
    <span style=color:#66d9ef>return</span> beta
 
 
beta <span style=color:#f92672>=</span> backward(V, a, b)
print(beta)
</code></pre></div><p><strong>R code</strong></p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r>data <span style=color:#f92672>=</span> <span style=color:#a6e22e>read.csv</span>(<span style=color:#e6db74>&#34;data_r.csv&#34;</span>)
 
a <span style=color:#f92672>=</span> <span style=color:#a6e22e>matrix</span>(<span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>0.54</span>, <span style=color:#ae81ff>0.49</span>, <span style=color:#ae81ff>0.46</span>, <span style=color:#ae81ff>0.51</span>),nrow <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>,ncol <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>)
b <span style=color:#f92672>=</span> <span style=color:#a6e22e>matrix</span>(<span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>0.16</span>, <span style=color:#ae81ff>0.25</span>, <span style=color:#ae81ff>0.26</span>, <span style=color:#ae81ff>0.28</span>, <span style=color:#ae81ff>0.58</span>, <span style=color:#ae81ff>0.47</span>),nrow <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>,ncol <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>)
 
backward <span style=color:#f92672>=</span> <span style=color:#a6e22e>function</span>(V, A, B){
  T <span style=color:#f92672>=</span> <span style=color:#a6e22e>length</span>(V)
  m <span style=color:#f92672>=</span> <span style=color:#a6e22e>nrow</span>(A)
  beta <span style=color:#f92672>=</span> <span style=color:#a6e22e>matrix</span>(<span style=color:#ae81ff>1</span>, T, m)
  
  <span style=color:#a6e22e>for</span>(t <span style=color:#a6e22e>in </span>(T<span style=color:#ae81ff>-1</span>)<span style=color:#f92672>:</span><span style=color:#ae81ff>1</span>){
    tmp <span style=color:#f92672>=</span> <span style=color:#a6e22e>as.matrix</span>(beta[t<span style=color:#ae81ff>+1</span>, ] <span style=color:#f92672>*</span> B[, V[t<span style=color:#ae81ff>+1</span>]])
    beta[t, ] <span style=color:#f92672>=</span> <span style=color:#a6e22e>t</span>(A <span style=color:#f92672>%*%</span> tmp)
  }
  <span style=color:#a6e22e>return</span>(beta)
}
 
<span style=color:#a6e22e>backward</span>(data<span style=color:#f92672>$</span>Visible,a,b)
</code></pre></div><p><strong>Output</strong></p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>[[<span style=color:#ae81ff>5.30694627e-221</span> <span style=color:#ae81ff>5.32373319e-221</span>]
 [<span style=color:#ae81ff>1.98173335e-220</span> <span style=color:#ae81ff>1.96008747e-220</span>]
 [<span style=color:#ae81ff>3.76013005e-220</span> <span style=color:#ae81ff>3.71905927e-220</span>]
 [<span style=color:#ae81ff>7.13445025e-220</span> <span style=color:#ae81ff>7.05652279e-220</span>]
<span style=color:#f92672>...</span>
<span style=color:#f92672>...</span>
 [<span style=color:#ae81ff>7.51699476e-002</span> <span style=color:#ae81ff>7.44006456e-002</span>]
 [<span style=color:#ae81ff>1.41806080e-001</span> <span style=color:#ae81ff>1.42258480e-001</span>]
 [<span style=color:#ae81ff>5.29400000e-001</span> <span style=color:#ae81ff>5.23900000e-001</span>]
 [<span style=color:#ae81ff>1.00000000e+000</span> <span style=color:#ae81ff>1.00000000e+000</span>]]
</code></pre></div><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1>
<p>In the next article we will use both forward and backward algorithm to solve the <code>learning problem</code>. Here we have provided a very detailed overview of the Forward and Backward Algorithm.
The output of the program may not make a lot of sense now however the next article will provide more insight.</p>
</div>
<footer class=post-footer>
<nav class=paginav>
<a class=prev href=/posts/introduction-to-time-series-forecasting/>
<span class=title>« Prev Page</span>
<br>
<span>Introduction to Time Series Forecasting</span>
</a>
<a class=next href=/posts/introduction-to-hidden-markov-model/>
<span class=title>Next Page »</span>
<br>
<span>Introduction to Hidden Markov Model</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Forward and Backward Algorithm in Hidden Markov Model on twitter" href="https://twitter.com/intent/tweet/?text=Forward%20and%20Backward%20Algorithm%20in%20Hidden%20Markov%20Model&url=%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Forward and Backward Algorithm in Hidden Markov Model on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f&title=Forward%20and%20Backward%20Algorithm%20in%20Hidden%20Markov%20Model&summary=Forward%20and%20Backward%20Algorithm%20in%20Hidden%20Markov%20Model&source=%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Forward and Backward Algorithm in Hidden Markov Model on reddit" href="https://reddit.com/submit?url=%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f&title=Forward%20and%20Backward%20Algorithm%20in%20Hidden%20Markov%20Model"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Forward and Backward Algorithm in Hidden Markov Model on facebook" href="https://facebook.com/sharer/sharer.php?u=%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Forward and Backward Algorithm in Hidden Markov Model on whatsapp" href="https://api.whatsapp.com/send?text=Forward%20and%20Backward%20Algorithm%20in%20Hidden%20Markov%20Model%20-%20%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Forward and Backward Algorithm in Hidden Markov Model on telegram" href="https://telegram.me/share/url?text=Forward%20and%20Backward%20Algorithm%20in%20Hidden%20Markov%20Model&url=%2fposts%2fforward-and-backward-algorithm-in-hidden-markov-model%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href>Marcin Halupka</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+="has-jax"})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</body>
</html>