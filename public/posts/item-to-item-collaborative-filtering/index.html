<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Item to Item Collaborative Filtering | Marcin Halupka</title>
<meta name=keywords content>
<meta name=description content="Recommendation algorithms are best known for their use on e-commerce Websites where they use input about a customer&rsquo;s interest to generate a list of recommended items. Many applications use only the items that customers purchase and explicitly rate to represent their interests but they can also use other attributes, including items viewed, demographic data, subject interests and favorite artists.
In 2003 Amazon published an industry report presenting their approach to that topic back then, this post will be its summary.">
<meta name=author content="Marcin Halupka">
<link rel=canonical href=/posts/item-to-item-collaborative-filtering/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css integrity="sha256-b2AFbUTT9+tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=16x16 href=%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=%3Clink%20/%20abs%20url%3E>
<link rel=apple-touch-icon href=%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="Item to Item Collaborative Filtering">
<meta property="og:description" content="Recommendation algorithms are best known for their use on e-commerce Websites where they use input about a customer&rsquo;s interest to generate a list of recommended items. Many applications use only the items that customers purchase and explicitly rate to represent their interests but they can also use other attributes, including items viewed, demographic data, subject interests and favorite artists.
In 2003 Amazon published an industry report presenting their approach to that topic back then, this post will be its summary.">
<meta property="og:type" content="article">
<meta property="og:url" content="/posts/item-to-item-collaborative-filtering/"><meta property="og:image" content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-11-16T17:15:59+01:00">
<meta property="article:modified_time" content="2021-11-16T17:15:59+01:00"><meta property="og:site_name" content="ExampleSite">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="Item to Item Collaborative Filtering">
<meta name=twitter:description content="Recommendation algorithms are best known for their use on e-commerce Websites where they use input about a customer&rsquo;s interest to generate a list of recommended items. Many applications use only the items that customers purchase and explicitly rate to represent their interests but they can also use other attributes, including items viewed, demographic data, subject interests and favorite artists.
In 2003 Amazon published an industry report presenting their approach to that topic back then, this post will be its summary.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"/posts/"},{"@type":"ListItem","position":3,"name":"Item to Item Collaborative Filtering","item":"/posts/item-to-item-collaborative-filtering/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Item to Item Collaborative Filtering","name":"Item to Item Collaborative Filtering","description":"Recommendation algorithms are best known for their use on e-commerce Websites where they use input about a customer\u0026rsquo;s interest to generate a list of recommended items. Many applications use only the items that customers purchase and explicitly rate to represent their interests but they can also use other attributes, including items viewed, demographic data, subject interests and favorite artists.\nIn 2003 Amazon published an industry report presenting their approach to that topic back then, this post will be its summary.","keywords":[],"articleBody":"Recommendation algorithms are best known for their use on e-commerce Websites where they use input about a customer’s interest to generate a list of recommended items. Many applications use only the items that customers purchase and explicitly rate to represent their interests but they can also use other attributes, including items viewed, demographic data, subject interests and favorite artists.\nIn 2003 Amazon published an industry report presenting their approach to that topic back then, this post will be its summary.\nRecommendation algorithms A typical set of challenges each recommendation algorithm is facing:\n A large retailer might have huge amounts of data, tens of millions of customers and millions of distinct catalog items. Many applications require the results set to be returned in realtime, in no more than half a second, while still producing high-quality recommendations. New customers typically have extremely limited information, based on only a few purchases on product ratings. Older customers can have an overflow of information, based on thousands of purchases and ratings. Customer data is volatile: Each interaction provides valuable customer data and the algorithm must respond immediately to new information.  Most recommendation algorithms start by finding a set of customers whose purchased and rated items overlap the user’s purchased and rated items. The algorithm aggregates items from these similar customers, eliminates items the user has already purchased or rated and recommends the remaining ones. Two popular versions of these algorithms are collaborative filtering and cluster models. Other algorithms - including search-based methods and Amazon’s item-to-item collaborative filtering focus on finding similar items not similar customers. For each of the user’s purchased and rated items, the algorithm attempts to find similar items then it aggregates and recommends them.\nTraditional Collaborative Filtering A traditional collaborative filtering algorithm represents a customer as an N-dimensional vector of items, where N is the number of distinct catalog items. The components of the vector are positive for purchased or positively rated items and negative for negatively rated items. To compensate for best-selling items, the algorithm typically multiplies the vector components by the inverse frequency (the inverse of the number of customers who have purchased or rated the item), making less well-known items much more relevant. For almost all customers, this vector is extremely sparse.\nThe algorithm generates recommendations based on a few customers who are most similar to the user. It can measure the similarity of two customers A and B in various ways, a common one is to measure the cosine of the angle between the two vectors:\n$$ \\text{similarity}(\\vec{A}, \\vec{B}) = \\cos(\\vec{A}, \\vec{B}) = \\frac{\\vec{A} \\bullet \\vec{B}}{||\\vec{A}|| * ||\\vec{B}||} $$\nThe algorithm can select recommendations from the similar customers' items using various methods as well, a common technique is to rank each item according to how many similar customers purchased it.\nUsing collaborative filtering to generate recommendations is computationally expensive. It’s $\\mathcal{O}(MN)$ in the worst case where $M$ is the number of customers and $N$ is the number of product catalog items since it examines $M$ customers and up to $N$ items for each customer. However, because the average customer vector is extremely sparse, the algorithm’s performance tends to be closer to $\\mathcal{O}(M + N)$. Even so, for very large data sets - such as 10 million or more customers and 1 million or more catalog items - the algorithm encounters severe performance and scaling issues.\nIt’s possible to partially addres these scaling issues by reducting the data size. We can reduce $M$ by randomly sampling customers or discarding customers with few purchases and reduce $N$ by discarding very popular or unpopular items. It is also possible to reduce the number of items examind by a small, constant factor by partitioning the item space based on product category or subject classification. Dimensionality reduction techniques such as clustering and principal component analysis can reduce $M$ or $N$ by a large factor.\nUnfortunately, all these methods also reduce recommendation quality in several ways:\n If the algorithm examines only a small customer sample, the selected customers will be less similar to the user. Item-space partitioning restricts recommendations to a specific product or subject area. If the algorithm discards the most popular or unpopular items, they will never appear as recommendations and customers who have purchased only those items will not get recommendations. Dimensionality reduction techniques tend to have the same effect by eliminating low-frequenct items. And applying them to customer space effectively groups similar customers into clusters, such clustering can also degrade recommendation quality.  Cluster models To find customers who are similar to the user, cluster models divide the customer base into many segments and treat the task as a classification problem. The algorithm’s goal is to assign the user to the segment containing the most similar customers. It then uses the purchases and ratings of the customers in the segment to generate recommendations.\nThe segments typically are created using a clustering or other unsupervised learning algorithm although some applications use manually determined segments. Based on a defined similarity metric most similar customers together form clusters or segments. As clustering over large data sets is impractical, most applications use various forms of greedy cluster generation. For very large data sets - especially those with high dimensionality - sampling or dimensionality reduction is necessary.\nOnce the algorithm generates segments, it computers the user’s similarity to vectors that summarize each segment, then chooses the segment with the strongest similarity and classifies user accordingly. Some algorithms classify users into multiple segments and describe the strength of each relationship.\nCluster models have better online scalability and performance than collaborative filtering because they compare the user to a controlled number of segments rather than the entire customer base. The complex and expensive clustering is run offline. However recommendation quality is low - quite often numerous customers in the segment are not the most similar customers and the recommendations they produce are less relevant. Using multiple fine-grained segments could improbe the quality but then the user-segment classification becomes almost as expensive as collaborative filtering.\nSearch-based methods Search- or content-based methods treat the recommendations problem as a search for related items. Given the user’s purchased and rated items, the algorithm constructs a search query to find other popular items by the same author, artist or director or with similar keywords or subjects. If a customer buys the Godfather DVD Collection, for example, the system might recommend other crime drama titles, other titles starring Marlon Brando, or other movies directed by Francis Ford Coppola.\nIf the user has few purchases or ratings, seach-based recommendation algorithms scale and perform well. For users with thousands of purchases, however, it’s impractical to base a query on all the items. The algorithm must use a subset or summary of the data, reducing quality. In all cases, recommendation quality is relatively poor. They are often either too general or too narrow. Recommendations should help a customer find and discover new, relevant and interesting items. Popular items by the same author or in the same subject category fail to achieve this goal.\nItem-to-item collaborative filtering Rather than matching the user to similar customers, item-to-item collaborative filtering matches each of the user’s purchased and rated items to similar items then combines those similar items into a recommendation list.\nTo determine the most-similar match for a given item, the algorithm builds a similar-items table by finding items that customers tend to purchase together. We could build a product-to-product matrix by iterating through all item pairs and computing a similarity metric for each pair. However many product pairs have no common customers so the approach is inefficient in terms of processing time and memory usage. The following iterative algorithm provides a better approach by calculating the similarity between a single product and all related products:\nFor each item in product catalog $I_1$\nFor each customer $C$ who purchased $I_1$\nFor each item $I_2$ purchased by customer $C$\nRecord that customer purchased $I_1$ and $I_2$\nFor each customer item $I_2$\nCompute the similarity between $I_1$ and $I_2$\nIt’s possible to compute the similarity between two items in various ways but a common method is to use the cosine measure that was described earlier in which each vector corresponds to an item rather than a customer and the vector’s $M$ dimensions correspond to customers who have purchased that item.\nThis offline computation of the similar-items table is extremely time intensive, with $\\mathcal{O}(N^2M)$ as worst case. In practice, however, it’s closer to $\\mathcal{O}(NM)$, as most customers have very few purchases. Sampling customers who purchase best-selling titles reduces runtime even further, with little reduction in quality.\nGiven a similar-items table, the algorithm finds items similar to each of the user’s purchases and ratings, aggregates those items, and then recommends the most popular or correlated items. This computation is very quick, depending only on the number of items the user purchased or rated.\nConclusion For large retailers like Amazon, recommendation algorithms provide an effective form of targeted marketing by creating a personalized shopping experience for each customer. At some point in time a good recommendation algorithm had to be scalable over very large customer bases and product catalogs, require only subsecond processing time to generate online recommendations, be able to react immediately to changes in a user’s data and make compelling recommendations for all users regardless of the number of purchases and ratings.\nThe key to item-to-item collaborative filtering’s scalability and performance was that it created the expensive similar-items table offline. The algorithm’s online component — looking up similar items for the user’s purchases and ratings — scaled independently of the catalog size or the total number of customers. It was dependent only on how many titles the user has purchased or rated. Thus, the algorithm was fast even for extremely large data sets. Because the algorithm recommended highly correlated similar items, recommendation quality was great and unlike traditional collaborative filtering, the algorithm also performed well with limited user data, producing reasonable recommendations based on as few as two or three items.\n","wordCount":"1658","inLanguage":"en","datePublished":"2021-11-16T17:15:59+01:00","dateModified":"2021-11-16T17:15:59+01:00","author":{"@type":"Person","name":"Marcin Halupka"},"mainEntityOfPage":{"@type":"WebPage","@id":"/posts/item-to-item-collaborative-filtering/"},"publisher":{"@type":"Organization","name":"Marcin Halupka","logo":{"@type":"ImageObject","url":"%3Clink%20/%20abs%20url%3E"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href accesskey=h title="Marcin Halupka (Alt + H)">Marcin Halupka</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=/posts/ title=Posts>
<span>Posts</span>
</a>
</li>
<li>
<a href=/projects/ title=Projects>
<span>Projects</span>
</a>
</li>
<li>
<a href=/about/ title="About me">
<span>About me</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href>Home</a>&nbsp;»&nbsp;<a href=/posts/>Posts</a></div>
<h1 class=post-title>
Item to Item Collaborative Filtering
</h1>
<div class=post-meta>November 16, 2021&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Marcin Halupka&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/item-to-item-collaborative-filtering.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header> <div class=toc>
<details open>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#recommendation-algorithms aria-label="Recommendation algorithms">Recommendation algorithms</a></li>
<li>
<a href=#traditional-collaborative-filtering aria-label="Traditional Collaborative Filtering">Traditional Collaborative Filtering</a></li>
<li>
<a href=#cluster-models aria-label="Cluster models">Cluster models</a></li>
<li>
<a href=#search-based-methods aria-label="Search-based methods">Search-based methods</a></li>
<li>
<a href=#item-to-item-collaborative-filtering aria-label="Item-to-item collaborative filtering">Item-to-item collaborative filtering</a></li>
<li>
<a href=#conclusion aria-label=Conclusion>Conclusion</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p>Recommendation algorithms are best known for their use on e-commerce Websites where they use input about a customer&rsquo;s interest to generate a list of recommended items.
Many applications use only the items that customers purchase and explicitly rate to represent their interests but they can also use other attributes, including items viewed, demographic data,
subject interests and favorite artists.</p>
<p>In 2003 Amazon published an industry report presenting their approach to that topic back then, this post will be its summary.</p>
<h1 id=recommendation-algorithms>Recommendation algorithms<a hidden class=anchor aria-hidden=true href=#recommendation-algorithms>#</a></h1>
<p>A typical set of challenges each recommendation algorithm is facing:</p>
<ul>
<li>A large retailer might have huge amounts of data, tens of millions of customers and millions of distinct catalog items.</li>
<li>Many applications require the results set to be returned in realtime, in no more than half a second, while still producing high-quality recommendations.</li>
<li>New customers typically have extremely limited information, based on only a few purchases on product ratings.</li>
<li>Older customers can have an overflow of information, based on thousands of purchases and ratings.</li>
<li>Customer data is volatile: Each interaction provides valuable customer data and the algorithm must respond immediately to new information.</li>
</ul>
<p>Most recommendation algorithms start by finding a set of customers whose purchased and rated items overlap the user&rsquo;s purchased and rated items. The algorithm aggregates items from these
similar customers, eliminates items the user has already purchased or rated and recommends the remaining ones. Two popular versions of these algorithms are <code>collaborative filtering</code> and <code>cluster models</code>.
Other algorithms - including <code>search-based methods</code> and Amazon&rsquo;s <code>item-to-item collaborative filtering</code> focus on finding similar <em>items</em> not similar <em>customers</em>. For each of the user&rsquo;s purchased and rated items,
the algorithm attempts to find similar items then it aggregates and recommends them.</p>
<h1 id=traditional-collaborative-filtering>Traditional Collaborative Filtering<a hidden class=anchor aria-hidden=true href=#traditional-collaborative-filtering>#</a></h1>
<p>A traditional collaborative filtering algorithm represents a customer as an N-dimensional vector of items, where N is the number of distinct catalog items.
The components of the vector are positive for purchased or positively rated items and negative for negatively rated items. To compensate for best-selling items,
the algorithm typically multiplies the vector components by the inverse frequency (the inverse of the number of customers who have purchased or rated the item), making less well-known items
much more relevant. For almost all customers, this vector is extremely sparse.</p>
<p>The algorithm generates recommendations based on a few customers who are most similar to the user. It can measure the similarity of two customers A and B in various ways, a common one is to measure
the cosine of the angle between the two vectors:</p>
<p>$$
\text{similarity}(\vec{A}, \vec{B}) = \cos(\vec{A}, \vec{B}) = \frac{\vec{A} \bullet \vec{B}}{||\vec{A}|| * ||\vec{B}||}
$$</p>
<p>The algorithm can select recommendations from the similar customers' items using various methods as well, a common technique is to rank each item according to how many similar customers purchased it.</p>
<p>Using collaborative filtering to generate recommendations is computationally expensive. It&rsquo;s $\mathcal{O}(MN)$ in the worst case where $M$ is the number of customers and $N$ is the number of product
catalog items since it examines $M$ customers and up to $N$ items for each customer. However, because the average customer vector is extremely sparse, the algorithm&rsquo;s performance tends to be closer to
$\mathcal{O}(M + N)$. Even so, for very large data sets - such as 10 million or more customers and 1 million or more catalog items - the algorithm encounters severe performance and scaling issues.</p>
<p>It&rsquo;s possible to partially addres these scaling issues by reducting the data size. We can reduce $M$ by randomly sampling customers or discarding customers with few purchases and reduce $N$ by discarding very popular
or unpopular items. It is also possible to reduce the number of items examind by a small, constant factor by partitioning the item space based on product category or subject classification. Dimensionality reduction techniques
such as clustering and principal component analysis can reduce $M$ or $N$ by a large factor.</p>
<p>Unfortunately, all these methods also reduce recommendation quality in several ways:</p>
<ul>
<li>If the algorithm examines only a small customer sample, the selected customers will be less similar to the user.</li>
<li>Item-space partitioning restricts recommendations to a specific product or subject area.</li>
<li>If the algorithm discards the most popular or unpopular items, they will never appear as recommendations and customers who have purchased only those items will not get recommendations.</li>
<li>Dimensionality reduction techniques tend to have the same effect by eliminating low-frequenct items.</li>
<li>And applying them to customer space effectively groups similar customers into clusters, such clustering can also degrade recommendation quality.</li>
</ul>
<h1 id=cluster-models>Cluster models<a hidden class=anchor aria-hidden=true href=#cluster-models>#</a></h1>
<p>To find customers who are similar to the user, cluster models divide the customer base into many segments and treat the task as a classification problem. The algorithm&rsquo;s goal is to assign the user to the segment
containing the most similar customers. It then uses the purchases and ratings of the customers in the segment to generate recommendations.</p>
<p>The segments typically are created using a clustering or other unsupervised learning algorithm although some applications use manually determined segments. Based on a defined similarity metric most similar customers together
form clusters or segments. As clustering over large data sets is impractical, most applications use various forms of greedy cluster generation. For very large data sets - especially those with high dimensionality - sampling
or dimensionality reduction is necessary.</p>
<p>Once the algorithm generates segments, it computers the user&rsquo;s similarity to vectors that summarize each segment, then chooses the segment with the strongest similarity and classifies user accordingly. Some algorithms
classify users into multiple segments and describe the strength of each relationship.</p>
<p>Cluster models have better online scalability and performance than collaborative filtering because they compare the user to a controlled number of segments rather than the entire customer base. The complex and expensive
clustering is run offline. However recommendation quality is low - quite often numerous customers in the segment are not the most similar customers and the recommendations they produce are less relevant. Using multiple
fine-grained segments could improbe the quality but then the user-segment classification becomes almost as expensive as collaborative filtering.</p>
<h1 id=search-based-methods>Search-based methods<a hidden class=anchor aria-hidden=true href=#search-based-methods>#</a></h1>
<p>Search- or content-based methods treat the recommendations problem as a search for related items. Given the user&rsquo;s purchased and rated items, the algorithm constructs a search query to find other popular items by the same
author, artist or director or with similar keywords or subjects. If a customer buys the Godfather DVD Collection, for example, the system might recommend other crime drama titles, other titles starring Marlon Brando, or other
movies directed by Francis Ford Coppola.</p>
<p>If the user has few purchases or ratings, seach-based recommendation algorithms scale and perform well. For users with thousands of purchases, however, it&rsquo;s impractical to base a query on all the items. The algorithm must
use a subset or summary of the data, reducing quality. In all cases, recommendation quality is relatively poor. They are often either too general or too narrow. Recommendations should help a customer find and discover new,
relevant and interesting items. Popular items by the same author or in the same subject category fail to achieve this goal.</p>
<h1 id=item-to-item-collaborative-filtering>Item-to-item collaborative filtering<a hidden class=anchor aria-hidden=true href=#item-to-item-collaborative-filtering>#</a></h1>
<p>Rather than matching the user to similar customers, item-to-item collaborative filtering matches each of the user&rsquo;s purchased and rated items to similar items then combines those similar items into a recommendation list.</p>
<p>To determine the most-similar match for a given item, the algorithm builds a similar-items table by finding items that customers tend to purchase together. We could build a product-to-product matrix by iterating through all
item pairs and computing a similarity metric for each pair. However many product pairs have no common customers so the approach is inefficient in terms of processing time and memory usage. The following iterative algorithm
provides a better approach by calculating the similarity between a single product and all related products:</p>
<p><em>For each item in product catalog $I_1$</em><br>
    <em>For each customer $C$ who purchased $I_1$</em><br>
        <em>For each item $I_2$ purchased by customer $C$</em><br>
            <em>Record that customer purchased $I_1$ and $I_2$</em><br>
    <em>For each customer item $I_2$</em><br>
        <em>Compute the similarity between $I_1$ and $I_2$</em></p>
<p>It&rsquo;s possible to compute the similarity between two items in various ways but a common method is to use the cosine measure that was described earlier in which each vector corresponds to an item rather than a customer and
the vector&rsquo;s $M$ dimensions correspond to customers who have purchased that item.</p>
<p>This offline computation of the similar-items table is extremely time intensive, with $\mathcal{O}(N^2M)$ as worst case. In practice, however, it’s closer to
$\mathcal{O}(NM)$, as most customers have very few purchases. Sampling customers who purchase best-selling titles reduces runtime even further, with little reduction in quality.</p>
<p>Given a similar-items table, the algorithm finds items similar to each of the user’s purchases and ratings, aggregates those items, and then recommends the most popular or correlated items.
This computation is very quick, depending only on the number of items the user purchased or rated.</p>
<h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1>
<p>For large retailers like Amazon, recommendation algorithms provide an effective form of targeted marketing by creating a personalized shopping experience for each customer. At some point in time
a good recommendation algorithm had to be scalable over very large customer bases and product catalogs, require only subsecond processing time to generate online recommendations,
be able to react immediately to changes in a user’s data and make compelling recommendations for all users regardless of the number of purchases and ratings.</p>
<p>The key to item-to-item collaborative filtering’s scalability and performance was that it created the expensive similar-items table offline. The algorithm’s
online component — looking up similar items for the user’s purchases and ratings — scaled independently of the catalog size or the total number of customers. It was dependent only on how
many titles the user has purchased or rated. Thus, the algorithm was fast even for extremely large data sets. Because the algorithm recommended highly correlated similar items, recommendation quality was great and
unlike traditional collaborative filtering, the algorithm also performed well with limited user data, producing reasonable recommendations based on as few as two or three items.</p>
</div>
<footer class=post-footer>
<nav class=paginav>
<a class=next href=/posts/marketing-mix-modeling/>
<span class=title>Next Page »</span>
<br>
<span>Marketing Mix Modeling</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Item to Item Collaborative Filtering on twitter" href="https://twitter.com/intent/tweet/?text=Item%20to%20Item%20Collaborative%20Filtering&url=%2fposts%2fitem-to-item-collaborative-filtering%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Item to Item Collaborative Filtering on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=%2fposts%2fitem-to-item-collaborative-filtering%2f&title=Item%20to%20Item%20Collaborative%20Filtering&summary=Item%20to%20Item%20Collaborative%20Filtering&source=%2fposts%2fitem-to-item-collaborative-filtering%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Item to Item Collaborative Filtering on reddit" href="https://reddit.com/submit?url=%2fposts%2fitem-to-item-collaborative-filtering%2f&title=Item%20to%20Item%20Collaborative%20Filtering"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Item to Item Collaborative Filtering on facebook" href="https://facebook.com/sharer/sharer.php?u=%2fposts%2fitem-to-item-collaborative-filtering%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Item to Item Collaborative Filtering on whatsapp" href="https://api.whatsapp.com/send?text=Item%20to%20Item%20Collaborative%20Filtering%20-%20%2fposts%2fitem-to-item-collaborative-filtering%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Item to Item Collaborative Filtering on telegram" href="https://telegram.me/share/url?text=Item%20to%20Item%20Collaborative%20Filtering&url=%2fposts%2fitem-to-item-collaborative-filtering%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href>Marcin Halupka</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+="has-jax"})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</body>
</html>